{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datelets/datalet2.csv',index_col=False)\n",
    "df = df.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period0</th>\n",
       "      <th>period1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999993</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999994</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999995</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999996</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999997</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         period0  period1\n",
       "0              3        2\n",
       "1              2        0\n",
       "2              0        2\n",
       "3              2        1\n",
       "4              1        1\n",
       "...          ...      ...\n",
       "9999993        3        3\n",
       "9999994        3        2\n",
       "9999995        2        0\n",
       "9999996        0        3\n",
       "9999997        3        2\n",
       "\n",
       "[9999998 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = []\n",
    "\n",
    "for i in range(df.shape[1]-1):\n",
    "\tdataheaders.append('period{}'.format(i))\n",
    " \n",
    "\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['period{}'.format(df.shape[1]-1)].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.],\n",
      "        [2.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [2.],\n",
      "        [0.],\n",
      "        [3.]])\n",
      "tensor([2, 0, 2,  ..., 0, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.01)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "#given that we are working with 10mil data points this is essential or we would simply run out of memory on the devices. Im using 2048 in the hope there are some gains to be made with that matrix size.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 15\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True, num_workers=12)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,4)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.hidden1    = nn.Linear(4,4)\n",
    "\t\tself.bnorm1 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden2    = nn.Linear(4,4)\n",
    "\t\tself.bnorm2 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden3    = nn.Linear(4,4)\n",
    "\t\tself.bnorm3 = nn.BatchNorm1d(4)\n",
    "\t\tself.hidden4    = nn.Linear(4,4)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(4,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x)\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel():\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose cross entropy loss as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont overfit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 24.5580901 \n",
      "epoch 1 done at time 39.7459773 \n",
      "epoch 2 done at time 54.9671812 \n",
      "epoch 3 done at time 70.0421831 \n",
      "epoch 4 done at time 85.189441 \n",
      "epoch 5 done at time 99.7885461 \n",
      "epoch 6 done at time 114.9850714 \n",
      "epoch 7 done at time 129.9903293 \n",
      "epoch 8 done at time 145.0399409 \n",
      "epoch 9 done at time 160.2119839 \n",
      "epoch 10 done at time 174.9092214 \n",
      "epoch 11 done at time 190.1204395 \n",
      "epoch 12 done at time 205.1167541 \n",
      "epoch 13 done at time 220.2692144 \n",
      "epoch 14 done at time 235.42516 \n",
      "epoch 15 done at time 250.5907506 \n",
      "epoch 16 done at time 265.6096892 \n",
      "epoch 17 done at time 280.4527107 \n",
      "epoch 18 done at time 295.5477572 \n",
      "epoch 19 done at time 310.5697066 \n",
      "epoch 20 done at time 325.6049222 \n",
      "epoch 21 done at time 340.678276 \n",
      "epoch 22 done at time 355.8538422 \n",
      "epoch 23 done at time 370.5794382 \n",
      "epoch 24 done at time 385.2178291 \n",
      "epoch 25 done at time 400.0014445 \n",
      "epoch 26 done at time 415.5617141 \n",
      "epoch 27 done at time 430.8275271 \n",
      "epoch 28 done at time 446.3468934 \n",
      "epoch 29 done at time 461.8706418 \n",
      "epoch 30 done at time 477.5850777 \n",
      "epoch 31 done at time 493.4239786 \n",
      "epoch 32 done at time 509.8933074 \n",
      "epoch 33 done at time 525.9806922 \n",
      "epoch 34 done at time 541.482851 \n",
      "epoch 35 done at time 556.4311104 \n",
      "epoch 36 done at time 572.1285893 \n",
      "epoch 37 done at time 587.4834992 \n",
      "epoch 38 done at time 603.0284294 \n",
      "epoch 39 done at time 618.4393247 \n",
      "epoch 40 done at time 634.0059131 \n",
      "epoch 41 done at time 649.6438973 \n",
      "epoch 42 done at time 664.9360577 \n",
      "epoch 43 done at time 680.403611 \n",
      "epoch 44 done at time 695.6734801 \n",
      "epoch 45 done at time 711.1609752 \n",
      "epoch 46 done at time 727.3009645 \n",
      "epoch 47 done at time 743.2452255 \n",
      "epoch 48 done at time 758.7746919 \n",
      "epoch 49 done at time 774.4750384 \n",
      "epoch 50 done at time 790.5036508 \n",
      "epoch 51 done at time 805.9425373 \n",
      "epoch 52 done at time 821.9793455 \n",
      "epoch 53 done at time 837.4873858 \n",
      "epoch 54 done at time 853.302494 \n",
      "epoch 55 done at time 869.2317334 \n",
      "epoch 56 done at time 885.0497934 \n",
      "epoch 57 done at time 900.8491467 \n",
      "epoch 58 done at time 916.4536625 \n",
      "epoch 59 done at time 932.2377531 \n",
      "epoch 60 done at time 947.898243 \n",
      "epoch 61 done at time 963.9550087 \n",
      "epoch 62 done at time 979.322166 \n",
      "epoch 63 done at time 995.0618397 \n",
      "epoch 64 done at time 1010.9216553 \n",
      "epoch 65 done at time 1026.1694138 \n",
      "epoch 66 done at time 1042.1406824 \n",
      "epoch 67 done at time 1057.709889 \n",
      "epoch 68 done at time 1073.5132233 \n",
      "epoch 69 done at time 1088.9498537 \n",
      "epoch 70 done at time 1104.7887979 \n",
      "epoch 71 done at time 1120.1997133 \n",
      "epoch 72 done at time 1136.1238304 \n",
      "epoch 73 done at time 1152.074734 \n",
      "epoch 74 done at time 1167.5555336 \n",
      "epoch 75 done at time 1183.4902214 \n",
      "epoch 76 done at time 1199.4715468 \n",
      "epoch 77 done at time 1215.3575294 \n",
      "epoch 78 done at time 1231.1548038 \n",
      "epoch 79 done at time 1246.9306643 \n",
      "epoch 80 done at time 1262.3214742 \n",
      "epoch 81 done at time 1278.3142368 \n",
      "epoch 82 done at time 1294.6114323 \n",
      "epoch 83 done at time 1310.6505947 \n",
      "epoch 84 done at time 1326.9483766 \n",
      "epoch 85 done at time 1342.7252775 \n",
      "epoch 86 done at time 1358.4242161 \n",
      "epoch 87 done at time 1373.8421488 \n",
      "epoch 88 done at time 1389.6362086 \n",
      "epoch 89 done at time 1405.3153958 \n",
      "epoch 90 done at time 1421.0306343 \n",
      "epoch 91 done at time 1436.8015533 \n",
      "epoch 92 done at time 1452.7645417 \n",
      "epoch 93 done at time 1468.434432 \n",
      "epoch 94 done at time 1484.1049184 \n",
      "epoch 95 done at time 1499.0909877 \n",
      "epoch 96 done at time 1514.6820536 \n",
      "epoch 97 done at time 1530.1297388 \n",
      "epoch 98 done at time 1545.4098543 \n",
      "epoch 99 done at time 1560.588576 \n",
      "epoch 100 done at time 1576.0082783 \n",
      "epoch 101 done at time 1591.2410316 \n",
      "epoch 102 done at time 1606.677196 \n",
      "epoch 103 done at time 1622.2173611 \n",
      "epoch 104 done at time 1637.4576283 \n",
      "epoch 105 done at time 1652.2751726 \n",
      "epoch 106 done at time 1667.8235111 \n",
      "epoch 107 done at time 1682.6663625 \n",
      "epoch 108 done at time 1698.0231033 \n",
      "epoch 109 done at time 1713.3723552 \n",
      "epoch 110 done at time 1728.9600227 \n",
      "epoch 111 done at time 1744.1848394 \n",
      "epoch 112 done at time 1759.4108855 \n",
      "epoch 113 done at time 1774.8196569 \n",
      "epoch 114 done at time 1790.1137348 \n",
      "epoch 115 done at time 1805.6081176 \n",
      "epoch 116 done at time 1820.6280489 \n",
      "epoch 117 done at time 1836.1311194 \n",
      "epoch 118 done at time 1851.1382734 \n",
      "epoch 119 done at time 1866.2999797 \n",
      "epoch 120 done at time 1881.4081508 \n",
      "epoch 121 done at time 1896.7508275 \n",
      "epoch 122 done at time 1911.8863249 \n",
      "epoch 123 done at time 1927.3431316 \n",
      "epoch 124 done at time 1942.4922776 \n",
      "epoch 125 done at time 1957.8708309 \n",
      "epoch 126 done at time 1973.3613237 \n",
      "epoch 127 done at time 1988.7326597 \n",
      "epoch 128 done at time 2004.2580919 \n",
      "epoch 129 done at time 2019.5700781 \n",
      "epoch 130 done at time 2034.9209079 \n",
      "epoch 131 done at time 2050.4899484 \n",
      "epoch 132 done at time 2065.8229893 \n",
      "epoch 133 done at time 2081.1457893 \n",
      "epoch 134 done at time 2096.7169158 \n",
      "epoch 135 done at time 2112.2077229 \n",
      "epoch 136 done at time 2127.6644472 \n",
      "epoch 137 done at time 2142.7058468 \n",
      "epoch 138 done at time 2157.9558566 \n",
      "epoch 139 done at time 2173.1430257 \n",
      "epoch 140 done at time 2188.5220233 \n",
      "epoch 141 done at time 2203.7933074 \n",
      "epoch 142 done at time 2219.1102489 \n",
      "epoch 143 done at time 2234.4085017 \n",
      "epoch 144 done at time 2249.9917048 \n",
      "epoch 145 done at time 2265.7531548 \n",
      "epoch 146 done at time 2281.088262 \n",
      "epoch 147 done at time 2296.3531143 \n",
      "epoch 148 done at time 2311.6712641 \n",
      "epoch 149 done at time 2326.8819336 \n",
      "epoch 150 done at time 2342.3038939 \n",
      "epoch 151 done at time 2357.1978124 \n",
      "epoch 152 done at time 2372.7590477 \n",
      "epoch 153 done at time 2388.1382175 \n",
      "epoch 154 done at time 2403.5542555 \n",
      "epoch 155 done at time 2418.9509134 \n",
      "epoch 156 done at time 2434.5197954 \n",
      "epoch 157 done at time 2449.5759623 \n",
      "epoch 158 done at time 2465.1969838 \n",
      "epoch 159 done at time 2480.4514031 \n",
      "epoch 160 done at time 2495.8360696 \n",
      "epoch 161 done at time 2511.26843 \n",
      "epoch 162 done at time 2526.6656281 \n",
      "epoch 163 done at time 2542.1705897 \n",
      "epoch 164 done at time 2557.2674266 \n",
      "epoch 165 done at time 2572.6393519 \n",
      "epoch 166 done at time 2588.1039978 \n",
      "epoch 167 done at time 2603.4988286 \n",
      "epoch 168 done at time 2619.1841635 \n",
      "epoch 169 done at time 2634.5539861 \n",
      "epoch 170 done at time 2650.1082269 \n",
      "epoch 171 done at time 2665.2951385 \n",
      "epoch 172 done at time 2680.5469086 \n",
      "epoch 173 done at time 2696.2361287 \n",
      "epoch 174 done at time 2711.1823823 \n",
      "epoch 175 done at time 2726.1411707 \n",
      "epoch 176 done at time 2741.4365925 \n",
      "epoch 177 done at time 2756.9231146 \n",
      "epoch 178 done at time 2772.2420605 \n",
      "epoch 179 done at time 2787.6064191 \n",
      "epoch 180 done at time 2803.2401915 \n",
      "epoch 181 done at time 2818.5207465 \n",
      "epoch 182 done at time 2833.7726502 \n",
      "epoch 183 done at time 2849.1182976 \n",
      "epoch 184 done at time 2864.4852493 \n",
      "epoch 185 done at time 2879.7696972 \n",
      "epoch 186 done at time 2894.9657917 \n",
      "epoch 187 done at time 2910.4559254 \n",
      "epoch 188 done at time 2925.8634116 \n",
      "epoch 189 done at time 2941.0661471 \n",
      "epoch 190 done at time 2956.4224992 \n",
      "epoch 191 done at time 2971.9070352 \n",
      "epoch 192 done at time 2987.6004326 \n",
      "epoch 193 done at time 3003.9157167 \n",
      "epoch 194 done at time 3020.3073874 \n",
      "epoch 195 done at time 3037.9185428 \n",
      "epoch 196 done at time 3053.8780961 \n",
      "epoch 197 done at time 3069.8118329 \n",
      "epoch 198 done at time 3085.9113908 \n",
      "epoch 199 done at time 3101.411902 \n",
      "epoch 200 done at time 3117.4223294 \n",
      "epoch 201 done at time 3133.4485662 \n",
      "epoch 202 done at time 3149.3410663 \n",
      "epoch 203 done at time 3165.3806918 \n",
      "epoch 204 done at time 3181.239989 \n",
      "epoch 205 done at time 3198.6727312 \n",
      "epoch 206 done at time 3214.9539805 \n"
     ]
    }
   ],
   "source": [
    "input_dim = df.shape[1]-1\n",
    "output_dim = 4\n",
    "numofepochs = 1000\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epoch')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Accuracy epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
