{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Count the number of CPU cores available, this returns the number of threads, because most cpus have a thread count equal to twice their core count I have halved the count when multithreading\n",
    "# Set this number to one if you dont want to multi thread the process\n",
    "\n",
    "cpuCount = os.cpu_count()\n",
    "print(cpuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datelets/datalet2.csv',index_col=False)\n",
    "df = df.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period0</th>\n",
       "      <th>period1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      period0  period1\n",
       "0           1        0\n",
       "1           0        1\n",
       "2           1        3\n",
       "3           3        2\n",
       "4           2        3\n",
       "...       ...      ...\n",
       "9993        0        0\n",
       "9994        0        1\n",
       "9995        1        0\n",
       "9996        0        0\n",
       "9997        0        1\n",
       "\n",
       "[9998 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = []\n",
    "\n",
    "for i in range(df.shape[1]-1):\n",
    "\tdataheaders.append('period{}'.format(i))\n",
    " \n",
    "\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['period{}'.format(df.shape[1]-1)].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([0, 1, 3,  ..., 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.01)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 6\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batches,shuffle=True,drop_last=True, num_workers=int(cpuCount/2))\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,4)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.hidden1    = nn.Linear(4,4)\n",
    "\t\tself.bnorm1 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden2    = nn.Linear(4,4)\n",
    "\t\tself.bnorm2 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden3    = nn.Linear(4,4)\n",
    "\t\tself.bnorm3 = nn.BatchNorm1d(4)\n",
    "\t\tself.hidden4    = nn.Linear(4,4)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(4,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x)\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel():\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose cross entropy loss as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont overfit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 10.8465534 \n",
      "epoch 1 done at time 12.4781585 \n",
      "epoch 2 done at time 14.111722 \n",
      "epoch 3 done at time 15.7287169 \n",
      "epoch 4 done at time 17.316176 \n",
      "epoch 5 done at time 18.9855679 \n",
      "epoch 6 done at time 20.6600191 \n",
      "epoch 7 done at time 22.2471192 \n",
      "epoch 8 done at time 23.9650633 \n",
      "epoch 9 done at time 25.7874232 \n",
      "epoch 10 done at time 27.4530251 \n",
      "epoch 11 done at time 29.0602646 \n",
      "epoch 12 done at time 30.6723377 \n",
      "epoch 13 done at time 32.3323578 \n",
      "epoch 14 done at time 33.9239021 \n",
      "epoch 15 done at time 35.5583683 \n",
      "epoch 16 done at time 37.1484096 \n",
      "epoch 17 done at time 38.7722669 \n",
      "epoch 18 done at time 40.4143017 \n",
      "epoch 19 done at time 42.0673206 \n",
      "epoch 20 done at time 43.7340541 \n",
      "epoch 21 done at time 45.3734308 \n",
      "epoch 22 done at time 47.0090331 \n",
      "epoch 23 done at time 48.6047543 \n",
      "epoch 24 done at time 50.3447107 \n",
      "epoch 25 done at time 52.1696433 \n",
      "epoch 26 done at time 53.8996059 \n",
      "epoch 27 done at time 55.6044672 \n",
      "epoch 28 done at time 57.3765171 \n",
      "epoch 29 done at time 59.1003829 \n",
      "epoch 30 done at time 60.7114964 \n",
      "epoch 31 done at time 62.2998701 \n",
      "epoch 32 done at time 63.8904252 \n",
      "epoch 33 done at time 65.6477712 \n",
      "epoch 34 done at time 67.5107493 \n",
      "epoch 35 done at time 69.1778038 \n",
      "epoch 36 done at time 70.8170606 \n",
      "epoch 37 done at time 72.4532086 \n",
      "epoch 38 done at time 74.1566919 \n",
      "epoch 39 done at time 75.8268449 \n",
      "epoch 40 done at time 77.4761238 \n",
      "epoch 41 done at time 79.1969485 \n",
      "epoch 42 done at time 80.8568111 \n",
      "epoch 43 done at time 82.5827291 \n",
      "epoch 44 done at time 84.2731763 \n",
      "epoch 45 done at time 85.9024425 \n",
      "epoch 46 done at time 87.6037888 \n",
      "epoch 47 done at time 89.2971199 \n",
      "epoch 48 done at time 91.5418102 \n",
      "epoch 49 done at time 93.2302218 \n",
      "epoch 50 done at time 94.8587837 \n",
      "epoch 51 done at time 96.4745313 \n",
      "epoch 52 done at time 98.1686321 \n",
      "epoch 53 done at time 99.8356645 \n",
      "epoch 54 done at time 101.4851543 \n",
      "epoch 55 done at time 103.1514199 \n",
      "epoch 56 done at time 104.8443856 \n",
      "epoch 57 done at time 106.5307506 \n",
      "epoch 58 done at time 108.3197236 \n",
      "epoch 59 done at time 110.0306629 \n",
      "epoch 60 done at time 111.6734353 \n",
      "epoch 61 done at time 113.3147801 \n",
      "epoch 62 done at time 114.9291997 \n",
      "epoch 63 done at time 116.5701201 \n",
      "epoch 64 done at time 118.1766634 \n",
      "epoch 65 done at time 119.8037058 \n",
      "epoch 66 done at time 121.5939988 \n",
      "epoch 67 done at time 123.3655825 \n",
      "epoch 68 done at time 125.0325479 \n",
      "epoch 69 done at time 126.663582 \n",
      "epoch 70 done at time 128.3105508 \n",
      "epoch 71 done at time 129.9682023 \n",
      "epoch 72 done at time 131.6230084 \n",
      "epoch 73 done at time 133.298658 \n",
      "epoch 74 done at time 134.9061629 \n",
      "epoch 75 done at time 136.5392062 \n",
      "epoch 76 done at time 138.2691227 \n",
      "epoch 77 done at time 139.9915256 \n",
      "epoch 78 done at time 141.6172823 \n",
      "epoch 79 done at time 143.312673 \n",
      "epoch 80 done at time 144.9623401 \n",
      "epoch 81 done at time 146.6421075 \n",
      "epoch 82 done at time 148.2665605 \n",
      "epoch 83 done at time 149.933975 \n",
      "epoch 84 done at time 151.5789126 \n",
      "epoch 85 done at time 153.2289246 \n",
      "epoch 86 done at time 154.8433197 \n",
      "epoch 87 done at time 156.6578136 \n",
      "epoch 88 done at time 158.3081253 \n",
      "epoch 89 done at time 159.9917824 \n",
      "epoch 90 done at time 161.8021425 \n",
      "epoch 91 done at time 163.610827 \n",
      "epoch 92 done at time 165.2671945 \n",
      "epoch 93 done at time 166.9539962 \n",
      "epoch 94 done at time 168.6580306 \n",
      "epoch 95 done at time 170.3110577 \n",
      "epoch 96 done at time 172.0125542 \n",
      "epoch 97 done at time 173.6534863 \n",
      "epoch 98 done at time 175.3605945 \n",
      "epoch 99 done at time 177.0634025 \n",
      "epoch 100 done at time 178.7748912 \n",
      "epoch 101 done at time 180.4676552 \n",
      "epoch 102 done at time 182.128058 \n",
      "epoch 103 done at time 183.7780808 \n",
      "epoch 104 done at time 185.4039305 \n",
      "epoch 105 done at time 187.0441807 \n",
      "epoch 106 done at time 188.8353027 \n",
      "epoch 107 done at time 190.5051669 \n",
      "epoch 108 done at time 192.1693879 \n",
      "epoch 109 done at time 193.8035053 \n",
      "epoch 110 done at time 195.4417553 \n",
      "epoch 111 done at time 197.1901314 \n",
      "epoch 112 done at time 198.8428726 \n",
      "epoch 113 done at time 200.547645 \n",
      "epoch 114 done at time 202.2420832 \n",
      "epoch 115 done at time 203.9180607 \n",
      "epoch 116 done at time 205.5751624 \n",
      "epoch 117 done at time 207.285397 \n",
      "epoch 118 done at time 209.074815 \n",
      "epoch 119 done at time 210.9097232 \n",
      "epoch 120 done at time 212.6479934 \n",
      "epoch 121 done at time 214.3976942 \n",
      "epoch 122 done at time 216.1084402 \n",
      "epoch 123 done at time 217.7576683 \n",
      "epoch 124 done at time 219.3938047 \n",
      "epoch 125 done at time 221.0098704 \n",
      "epoch 126 done at time 222.6397306 \n",
      "epoch 127 done at time 224.4797921 \n",
      "epoch 128 done at time 226.0858721 \n",
      "epoch 129 done at time 227.6998661 \n",
      "epoch 130 done at time 229.291073 \n",
      "epoch 131 done at time 230.9220591 \n",
      "epoch 132 done at time 232.8272054 \n",
      "epoch 133 done at time 234.4263912 \n",
      "epoch 134 done at time 236.1813176 \n",
      "epoch 135 done at time 237.7860767 \n",
      "epoch 136 done at time 239.3995812 \n",
      "epoch 137 done at time 241.0381437 \n",
      "epoch 138 done at time 242.8256397 \n",
      "epoch 139 done at time 244.6115392 \n",
      "epoch 140 done at time 246.5548668 \n",
      "epoch 141 done at time 248.5175228 \n",
      "epoch 142 done at time 250.2759732 \n",
      "epoch 143 done at time 251.9756054 \n",
      "epoch 144 done at time 253.6373111 \n",
      "epoch 145 done at time 255.359252 \n",
      "epoch 146 done at time 257.0740382 \n",
      "epoch 147 done at time 258.7329851 \n",
      "epoch 148 done at time 260.4092253 \n",
      "epoch 149 done at time 262.0441191 \n",
      "epoch 150 done at time 263.9985266 \n",
      "epoch 151 done at time 265.7244246 \n",
      "epoch 152 done at time 267.4074379 \n",
      "epoch 153 done at time 269.1164475 \n",
      "epoch 154 done at time 270.6774827 \n",
      "epoch 155 done at time 272.2740985 \n",
      "epoch 156 done at time 273.9121617 \n",
      "epoch 157 done at time 275.5121089 \n",
      "epoch 158 done at time 277.1527412 \n",
      "epoch 159 done at time 279.4742407 \n",
      "epoch 160 done at time 281.2434782 \n",
      "epoch 161 done at time 282.8989692 \n",
      "epoch 162 done at time 284.5289862 \n",
      "epoch 163 done at time 286.2962009 \n",
      "epoch 164 done at time 287.9760523 \n",
      "epoch 165 done at time 289.6176118 \n",
      "epoch 166 done at time 291.2941321 \n",
      "epoch 167 done at time 292.9131969 \n",
      "epoch 168 done at time 294.894022 \n",
      "epoch 169 done at time 296.5301069 \n",
      "epoch 170 done at time 298.1786559 \n",
      "epoch 171 done at time 299.8171595 \n",
      "epoch 172 done at time 301.4595571 \n",
      "epoch 173 done at time 303.1226874 \n",
      "epoch 174 done at time 304.7615706 \n",
      "epoch 175 done at time 306.4392516 \n",
      "epoch 176 done at time 308.1061705 \n",
      "epoch 177 done at time 309.9888805 \n",
      "epoch 178 done at time 311.6676864 \n",
      "epoch 179 done at time 313.5382772 \n",
      "epoch 180 done at time 315.1794707 \n",
      "epoch 181 done at time 316.8119692 \n",
      "epoch 182 done at time 318.5370832 \n",
      "epoch 183 done at time 320.1770168 \n",
      "epoch 184 done at time 321.8078578 \n",
      "epoch 185 done at time 323.5006344 \n",
      "epoch 186 done at time 325.1649857 \n",
      "epoch 187 done at time 327.0354705 \n",
      "epoch 188 done at time 328.7684366 \n",
      "epoch 189 done at time 330.4339265 \n",
      "epoch 190 done at time 332.1274173 \n",
      "epoch 191 done at time 333.8388623 \n",
      "epoch 192 done at time 335.5351969 \n",
      "epoch 193 done at time 337.2003873 \n",
      "epoch 194 done at time 338.9630522 \n",
      "epoch 195 done at time 340.6270579 \n",
      "epoch 196 done at time 342.3647866 \n",
      "epoch 197 done at time 344.0590887 \n",
      "epoch 198 done at time 345.7429133 \n",
      "epoch 199 done at time 347.5235558 \n",
      "epoch 200 done at time 349.2202425 \n",
      "epoch 201 done at time 350.9248153 \n",
      "epoch 202 done at time 352.6766967 \n",
      "epoch 203 done at time 354.369257 \n",
      "epoch 204 done at time 356.1064413 \n",
      "epoch 205 done at time 357.7656196 \n",
      "epoch 206 done at time 359.5247274 \n",
      "epoch 207 done at time 361.4052177 \n",
      "epoch 208 done at time 363.2337305 \n",
      "epoch 209 done at time 365.090548 \n",
      "epoch 210 done at time 366.7948759 \n",
      "epoch 211 done at time 368.4887102 \n",
      "epoch 212 done at time 370.1792915 \n",
      "epoch 213 done at time 372.474762 \n",
      "epoch 214 done at time 374.167146 \n",
      "epoch 215 done at time 375.8684969 \n",
      "epoch 216 done at time 377.5306583 \n",
      "epoch 217 done at time 379.2033071 \n",
      "epoch 218 done at time 380.8977563 \n",
      "epoch 219 done at time 382.5757652 \n",
      "epoch 220 done at time 384.3799895 \n",
      "epoch 221 done at time 386.1032275 \n",
      "epoch 222 done at time 388.0178305 \n",
      "epoch 223 done at time 389.6929533 \n",
      "epoch 224 done at time 391.3797429 \n",
      "epoch 225 done at time 393.1210306 \n",
      "epoch 226 done at time 394.8215688 \n",
      "epoch 227 done at time 396.9109089 \n",
      "epoch 228 done at time 398.8650063 \n",
      "epoch 229 done at time 400.5730118 \n",
      "epoch 230 done at time 402.2664018 \n",
      "epoch 231 done at time 404.1345695 \n",
      "epoch 232 done at time 405.928734 \n",
      "epoch 233 done at time 407.7212641 \n",
      "epoch 234 done at time 409.4483108 \n",
      "epoch 235 done at time 411.9960816 \n",
      "epoch 236 done at time 413.7159011 \n",
      "epoch 237 done at time 415.5401805 \n",
      "epoch 238 done at time 417.6808899 \n",
      "epoch 239 done at time 419.5242554 \n",
      "epoch 240 done at time 421.3598639 \n",
      "epoch 241 done at time 423.204309 \n",
      "epoch 242 done at time 424.9856665 \n",
      "epoch 243 done at time 427.5429221 \n",
      "epoch 244 done at time 429.5754369 \n",
      "epoch 245 done at time 431.4525761 \n",
      "epoch 246 done at time 433.3142021 \n",
      "epoch 247 done at time 435.0890898 \n",
      "epoch 248 done at time 436.8032388 \n",
      "epoch 249 done at time 438.5175278 \n",
      "epoch 250 done at time 440.1838323 \n",
      "epoch 251 done at time 443.0697689 \n",
      "epoch 252 done at time 444.7768428 \n",
      "epoch 253 done at time 446.4730747 \n",
      "epoch 254 done at time 448.2614769 \n",
      "epoch 255 done at time 449.9250925 \n",
      "epoch 256 done at time 451.6562165 \n",
      "epoch 257 done at time 453.3224117 \n",
      "epoch 258 done at time 454.9968401 \n",
      "epoch 259 done at time 456.6341558 \n",
      "epoch 260 done at time 458.3179604 \n",
      "epoch 261 done at time 459.966786 \n",
      "epoch 262 done at time 461.6572313 \n",
      "epoch 263 done at time 463.3457879 \n",
      "epoch 264 done at time 465.0463473 \n",
      "epoch 265 done at time 466.7579376 \n",
      "epoch 266 done at time 468.4160487 \n",
      "epoch 267 done at time 470.1209574 \n",
      "epoch 268 done at time 471.8408854 \n",
      "epoch 269 done at time 473.5228109 \n",
      "epoch 270 done at time 475.6409109 \n",
      "epoch 271 done at time 477.3405696 \n",
      "epoch 272 done at time 479.0506981 \n",
      "epoch 273 done at time 480.7661071 \n",
      "epoch 274 done at time 482.4886542 \n",
      "epoch 275 done at time 485.0044964 \n",
      "epoch 276 done at time 486.6713369 \n",
      "epoch 277 done at time 488.3663684 \n",
      "epoch 278 done at time 490.1201411 \n",
      "epoch 279 done at time 491.7927574 \n",
      "epoch 280 done at time 493.4678508 \n",
      "epoch 281 done at time 495.1807486 \n",
      "epoch 282 done at time 497.1333542 \n",
      "epoch 283 done at time 498.8587071 \n",
      "epoch 284 done at time 500.6167944 \n",
      "epoch 285 done at time 502.4322598 \n",
      "epoch 286 done at time 504.1375384 \n",
      "epoch 287 done at time 506.010196 \n",
      "epoch 288 done at time 507.7847098 \n",
      "epoch 289 done at time 509.4744437 \n",
      "epoch 290 done at time 511.3241321 \n",
      "epoch 291 done at time 513.1141009 \n",
      "epoch 292 done at time 514.7876109 \n",
      "epoch 293 done at time 516.5195414 \n",
      "epoch 294 done at time 518.5935228 \n",
      "epoch 295 done at time 520.3145296 \n",
      "epoch 296 done at time 522.0003146 \n",
      "epoch 297 done at time 524.0128597 \n",
      "epoch 298 done at time 526.1663645 \n",
      "epoch 299 done at time 527.9651281 \n",
      "epoch 300 done at time 529.6845615 \n",
      "epoch 301 done at time 531.4486534 \n",
      "epoch 302 done at time 533.2838522 \n",
      "epoch 303 done at time 534.9547755 \n",
      "epoch 304 done at time 536.8405197 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5964/442615055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mOutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainAcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestAcc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainthemodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5964/1139830106.py\u001b[0m in \u001b[0;36mtrainthemodel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mtestAcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myHat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch {} done at time {} '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = df.shape[1]-1\n",
    "output_dim = 4\n",
    "numofepochs = 1000\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epoch')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Accuracy epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
