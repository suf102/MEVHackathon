{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Count the number of CPU cores available, this returns the number of threads, because most cpus have a thread count equal to twice their core count I have halved the count when multithreading\n",
    "# Set this number to one if you dont want to multi thread the process\n",
    "\n",
    "cpuCount = os.cpu_count()\n",
    "print(cpuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring in the files\n",
    "\n",
    "df = pd.read_csv('../datalets/datalet2.csv',index_col=False)\n",
    "df = df.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period0</th>\n",
       "      <th>period1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152900</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152901</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152902</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152903</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152904</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3152905 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         period0  period1\n",
       "0              4        0\n",
       "1              0        0\n",
       "2              0        0\n",
       "3              0        0\n",
       "4              0        0\n",
       "...          ...      ...\n",
       "3152900        4        0\n",
       "3152901        0        0\n",
       "3152902        0        4\n",
       "3152903        4        0\n",
       "3152904        0        4\n",
       "\n",
       "[3152905 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['period0'] = df['period0'].apply(lambda x: 'vhigh' if x == 4 else x )\n",
    "df['period0'] = df['period0'].apply(lambda x: 'high' if x == 3 else x )\n",
    "df['period0'] = df['period0'].apply(lambda x: 'med' if x == 2 else x)\n",
    "df['period0'] = df['period0'].apply(lambda x: 'low' if x == 1 else x)\n",
    "df['period0'] = df['period0'].apply(lambda x: 'none' if x == 0  else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period1</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>med</th>\n",
       "      <th>none</th>\n",
       "      <th>vhigh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152901</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152902</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152903</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152904</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3152905 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         period1  high  low  med  none  vhigh\n",
       "0              0     0    0    0     0      1\n",
       "1              0     0    0    0     1      0\n",
       "2              0     0    0    0     1      0\n",
       "3              0     0    0    0     1      0\n",
       "4              0     0    0    0     1      0\n",
       "...          ...   ...  ...  ...   ...    ...\n",
       "3152900        0     0    0    0     0      1\n",
       "3152901        0     0    0    0     1      0\n",
       "3152902        4     0    0    0     1      0\n",
       "3152903        0     0    0    0     0      1\n",
       "3152904        4     0    0    0     1      0\n",
       "\n",
       "[3152905 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am splitting the data into dummy columns rather than keeping it as a single value to help with the batch normalization and with the model training. not strictly necessary though.\n",
    "\n",
    "dummies = pd.get_dummies(df.period0)\n",
    "\n",
    "df = pd.concat([df, dummies], axis='columns')\n",
    "\n",
    "df.drop(['period0'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the datasets using pytorch. Notice that the data is stored as floats but the labels as a int(longtensor) this is because the loss function gets unhappy if you try to pass it a float for hte lables.\n",
    "# as it works out probabilities then compares them to the categorical variable. \n",
    "dataheaders = dummies.columns\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['period1'].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([0, 0, 0,  ..., 4, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.01)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 18\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batches,shuffle=True,drop_last=True,num_workers=int(cpuCount/2))\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model, I am going to use batch nomralisation to try and make the model \n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,4)\n",
    "\t\t\n",
    "\t\t### hidden layers and batch normalisation layers \n",
    "\t\tself.hidden1    = nn.Linear(4,4)\n",
    "\t\tself.bnorm1 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden2    = nn.Linear(4,4)\n",
    "\t\tself.bnorm2 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden3    = nn.Linear(4,4)\n",
    "\t\tself.bnorm3 = nn.BatchNorm1d(4)\n",
    "\t\tself.hidden4    = nn.Linear(4,4)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(4,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel(learning):\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose the cross entoply loss for the non binary catagorisation problem. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=learning)\n",
    "\t\n",
    "\t#initialize a way to store the losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "    \n",
    "\t\t\t#pass the data to the GPU, this seems to be the real bottle neck in training the model\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\t#bring this data back to the CPU to calculate the accuracy \n",
    "   \n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy calculations so we can make sure that we are actually improving the accuracy on the train set as we train the data\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont over fit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\t#this is small enough it could be done on the CPU but ah well. \n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "\n",
    "\t\t#back to the CPU to store the test acc data\n",
    "\n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "  \n",
    "\t\t#just timing the epochs so we can check the speed and be sure it is traing at a decent rate and nothing is going wrong.\n",
    "\t\t#Plus its nice to have some indication it is actually working\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 29.0865312 \n",
      "epoch 1 done at time 36.7329663 \n",
      "epoch 2 done at time 44.2808024 \n",
      "epoch 3 done at time 51.5817359 \n",
      "epoch 4 done at time 58.867857 \n",
      "epoch 5 done at time 65.9907207 \n",
      "epoch 6 done at time 73.4100435 \n",
      "epoch 7 done at time 80.5743959 \n",
      "epoch 8 done at time 87.9581788 \n",
      "epoch 9 done at time 95.3965537 \n",
      "epoch 10 done at time 102.9417463 \n",
      "epoch 11 done at time 110.2153165 \n",
      "epoch 12 done at time 117.3610122 \n",
      "epoch 13 done at time 124.9052705 \n",
      "epoch 14 done at time 132.5181211 \n",
      "epoch 15 done at time 140.1396187 \n",
      "epoch 16 done at time 147.7877073 \n",
      "epoch 17 done at time 155.1723548 \n",
      "epoch 18 done at time 162.5996426 \n",
      "epoch 19 done at time 170.0415806 \n",
      "epoch 20 done at time 177.4972503 \n",
      "epoch 21 done at time 184.7825095 \n",
      "epoch 22 done at time 192.5688266 \n",
      "epoch 23 done at time 200.0210197 \n",
      "epoch 24 done at time 207.4780581 \n",
      "epoch 25 done at time 214.7823438 \n",
      "epoch 26 done at time 222.2936129 \n",
      "epoch 27 done at time 229.7271877 \n",
      "epoch 28 done at time 237.4112258 \n",
      "epoch 29 done at time 245.7231637 \n",
      "epoch 30 done at time 253.8709029 \n",
      "epoch 31 done at time 261.510529 \n",
      "epoch 32 done at time 269.4791215 \n",
      "epoch 33 done at time 277.0724856 \n",
      "epoch 34 done at time 284.9191199 \n",
      "epoch 35 done at time 292.7135 \n",
      "epoch 36 done at time 300.6772565 \n",
      "epoch 37 done at time 308.5894038 \n",
      "epoch 38 done at time 316.6449123 \n",
      "epoch 39 done at time 325.3446277 \n",
      "epoch 40 done at time 333.5675956 \n",
      "epoch 41 done at time 341.4375958 \n",
      "epoch 42 done at time 349.4749448 \n",
      "epoch 43 done at time 357.3196046 \n",
      "epoch 44 done at time 365.1205389 \n",
      "epoch 45 done at time 373.2711332 \n",
      "epoch 46 done at time 381.2194109 \n",
      "epoch 47 done at time 389.21515 \n",
      "epoch 48 done at time 397.3878897 \n",
      "epoch 49 done at time 405.3152122 \n",
      "epoch 50 done at time 413.7962501 \n",
      "epoch 51 done at time 421.800598 \n",
      "epoch 52 done at time 429.7721142 \n",
      "epoch 53 done at time 437.6794584 \n",
      "epoch 54 done at time 445.6487988 \n",
      "epoch 55 done at time 453.2822275 \n",
      "epoch 56 done at time 460.9207643 \n",
      "epoch 57 done at time 468.6975225 \n",
      "epoch 58 done at time 476.3853696 \n",
      "epoch 59 done at time 484.002209 \n",
      "epoch 60 done at time 491.6791269 \n",
      "epoch 61 done at time 499.3040118 \n",
      "epoch 62 done at time 506.9771672 \n",
      "epoch 63 done at time 514.6122442 \n",
      "epoch 64 done at time 522.2012991 \n",
      "epoch 65 done at time 529.9552067 \n",
      "epoch 66 done at time 537.5564019 \n",
      "epoch 67 done at time 545.108309 \n",
      "epoch 68 done at time 552.7410531 \n",
      "epoch 69 done at time 560.5286366 \n",
      "epoch 70 done at time 568.4369634 \n",
      "epoch 71 done at time 576.3163609 \n",
      "epoch 72 done at time 584.418643 \n",
      "epoch 73 done at time 592.4306583 \n",
      "epoch 74 done at time 601.8116665 \n",
      "epoch 75 done at time 609.518758 \n",
      "epoch 76 done at time 617.3943389 \n",
      "epoch 77 done at time 625.3988568 \n",
      "epoch 78 done at time 633.6384618 \n",
      "epoch 79 done at time 642.0716986 \n",
      "epoch 80 done at time 650.4278233 \n",
      "epoch 81 done at time 658.5470782 \n",
      "epoch 82 done at time 666.7224063 \n",
      "epoch 83 done at time 674.7206464 \n",
      "epoch 84 done at time 682.782209 \n",
      "epoch 85 done at time 691.0160204 \n",
      "epoch 86 done at time 698.8254673 \n",
      "epoch 87 done at time 707.210418 \n",
      "epoch 88 done at time 715.2942308 \n",
      "epoch 89 done at time 722.9951014 \n",
      "epoch 90 done at time 730.7642587 \n",
      "epoch 91 done at time 738.6059732 \n",
      "epoch 92 done at time 746.5452937 \n",
      "epoch 93 done at time 754.4416727 \n",
      "epoch 94 done at time 762.5146087 \n",
      "epoch 95 done at time 770.4159858 \n",
      "epoch 96 done at time 778.3825919 \n",
      "epoch 97 done at time 786.5244852 \n",
      "epoch 98 done at time 794.5323601 \n",
      "epoch 99 done at time 802.3274211 \n",
      "epoch 100 done at time 810.2902394 \n",
      "epoch 101 done at time 818.2044153 \n",
      "epoch 102 done at time 825.9809929 \n",
      "epoch 103 done at time 833.7873126 \n",
      "epoch 104 done at time 841.7597453 \n",
      "epoch 105 done at time 849.7059034 \n",
      "epoch 106 done at time 857.473701 \n",
      "epoch 107 done at time 865.3882659 \n",
      "epoch 108 done at time 873.3345437 \n",
      "epoch 109 done at time 881.1251735 \n",
      "epoch 110 done at time 888.8056975 \n",
      "epoch 111 done at time 896.3859379 \n",
      "epoch 112 done at time 903.6414557 \n",
      "epoch 113 done at time 911.0004517 \n",
      "epoch 114 done at time 918.1937703 \n",
      "epoch 115 done at time 925.2291961 \n",
      "epoch 116 done at time 932.4754743 \n",
      "epoch 117 done at time 939.7587149 \n",
      "epoch 118 done at time 947.0131339 \n",
      "epoch 119 done at time 955.3209989 \n",
      "epoch 120 done at time 963.0624882 \n",
      "epoch 121 done at time 970.6666885 \n",
      "epoch 122 done at time 978.118151 \n",
      "epoch 123 done at time 985.3388528 \n",
      "epoch 124 done at time 992.4151288 \n",
      "epoch 125 done at time 999.5563353 \n",
      "epoch 126 done at time 1006.795827 \n",
      "epoch 127 done at time 1013.9150916 \n",
      "epoch 128 done at time 1021.2081828 \n",
      "epoch 129 done at time 1028.3573225 \n",
      "epoch 130 done at time 1035.512228 \n",
      "epoch 131 done at time 1042.8409603 \n",
      "epoch 132 done at time 1050.0826267 \n",
      "epoch 133 done at time 1057.5753118 \n",
      "epoch 134 done at time 1064.7834062 \n",
      "epoch 135 done at time 1072.0613696 \n",
      "epoch 136 done at time 1079.1380572 \n",
      "epoch 137 done at time 1086.247844 \n",
      "epoch 138 done at time 1093.5863856 \n",
      "epoch 139 done at time 1100.8181738 \n",
      "epoch 140 done at time 1108.0852935 \n",
      "epoch 141 done at time 1115.2112552 \n",
      "epoch 142 done at time 1122.4811388 \n",
      "epoch 143 done at time 1129.763394 \n",
      "epoch 144 done at time 1136.8669172 \n",
      "epoch 145 done at time 1143.9814763 \n",
      "epoch 146 done at time 1151.1927927 \n",
      "epoch 147 done at time 1158.3787166 \n",
      "epoch 148 done at time 1165.4217637 \n",
      "epoch 149 done at time 1172.6478372 \n",
      "epoch 150 done at time 1179.8910477 \n",
      "epoch 151 done at time 1187.1026288 \n",
      "epoch 152 done at time 1194.2468603 \n",
      "epoch 153 done at time 1201.5849815 \n",
      "epoch 154 done at time 1208.7493791 \n",
      "epoch 155 done at time 1215.9166637 \n",
      "epoch 156 done at time 1223.372751 \n",
      "epoch 157 done at time 1230.6953874 \n",
      "epoch 158 done at time 1237.78578 \n",
      "epoch 159 done at time 1244.954849 \n",
      "epoch 160 done at time 1252.1587675 \n",
      "epoch 161 done at time 1259.5074129 \n",
      "epoch 162 done at time 1266.7617643 \n",
      "epoch 163 done at time 1274.1194082 \n",
      "epoch 164 done at time 1281.2516696 \n",
      "epoch 165 done at time 1288.4666887 \n",
      "epoch 166 done at time 1295.5776317 \n",
      "epoch 167 done at time 1302.7814462 \n",
      "epoch 168 done at time 1309.8799226 \n",
      "epoch 169 done at time 1317.0459187 \n",
      "epoch 170 done at time 1324.1517508 \n",
      "epoch 171 done at time 1331.2652703 \n",
      "epoch 172 done at time 1338.6760007 \n",
      "epoch 173 done at time 1345.9043129 \n",
      "epoch 174 done at time 1353.1204617 \n",
      "epoch 175 done at time 1360.3323498 \n",
      "epoch 176 done at time 1367.6220358 \n",
      "epoch 177 done at time 1374.7828434 \n",
      "epoch 178 done at time 1381.9029313 \n",
      "epoch 179 done at time 1389.1324259 \n",
      "epoch 180 done at time 1396.383296 \n",
      "epoch 181 done at time 1403.586024 \n",
      "epoch 182 done at time 1410.8077754 \n",
      "epoch 183 done at time 1418.0011583 \n",
      "epoch 184 done at time 1425.3133077 \n",
      "epoch 185 done at time 1432.4675441 \n",
      "epoch 186 done at time 1439.7249641 \n",
      "epoch 187 done at time 1446.9095151 \n",
      "epoch 188 done at time 1454.1700946 \n",
      "epoch 189 done at time 1461.5205045 \n",
      "epoch 190 done at time 1468.9381739 \n",
      "epoch 191 done at time 1476.2320997 \n",
      "epoch 192 done at time 1483.4014047 \n",
      "epoch 193 done at time 1490.819037 \n",
      "epoch 194 done at time 1498.0737982 \n",
      "epoch 195 done at time 1505.2312099 \n",
      "epoch 196 done at time 1512.4194607 \n",
      "epoch 197 done at time 1519.6425905 \n",
      "epoch 198 done at time 1526.8166871 \n",
      "epoch 199 done at time 1533.9659441 \n"
     ]
    }
   ],
   "source": [
    "input_dim = dummies.shape[1]\n",
    "output_dim = 5\n",
    "numofepochs = 200\n",
    "learningrate = 0.01\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel(learningrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Just some plots to be sure that we are doing the things we want to do\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epochs')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Train, test accuracy over epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.29527258872986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the final accuracy, if using sample data, this should be around 25% as there is no structure to the data. \n",
    "\n",
    "# because its a small batch we can just run this on the CPU\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "100*torch.mean((torch.argmax(model(eval_data),axis=1) == eval_labels).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../Deeplearningmodels/markovchainappoximation.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
