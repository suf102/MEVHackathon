{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datelets/datalet9.csv',index_col=False)\n",
    "df = df.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period0</th>\n",
       "      <th>period1</th>\n",
       "      <th>period2</th>\n",
       "      <th>period3</th>\n",
       "      <th>period4</th>\n",
       "      <th>period5</th>\n",
       "      <th>period6</th>\n",
       "      <th>period7</th>\n",
       "      <th>period8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999986</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999987</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999988</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999989</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999990</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999991 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         period0  period1  period2  period3  period4  period5  period6  \\\n",
       "0              3        2        0        2        1        1        0   \n",
       "1              2        0        2        1        1        0        1   \n",
       "2              0        2        1        1        0        1        3   \n",
       "3              2        1        1        0        1        3        2   \n",
       "4              1        1        0        1        3        2        3   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "9999986        1        3        3        3        3        0        3   \n",
       "9999987        3        3        3        3        0        3        3   \n",
       "9999988        3        3        3        0        3        3        3   \n",
       "9999989        3        3        0        3        3        3        2   \n",
       "9999990        3        0        3        3        3        2        0   \n",
       "\n",
       "         period7  period8  \n",
       "0              1        3  \n",
       "1              3        2  \n",
       "2              2        3  \n",
       "3              3        1  \n",
       "4              1        0  \n",
       "...          ...      ...  \n",
       "9999986        3        3  \n",
       "9999987        3        2  \n",
       "9999988        2        0  \n",
       "9999989        0        3  \n",
       "9999990        3        2  \n",
       "\n",
       "[9999991 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = []\n",
    "\n",
    "for i in range(df.shape[1]-1):\n",
    "\tdataheaders.append('period{}'.format(i))\n",
    "\t\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).float()\n",
    "labels = torch.Tensor(df['period{}'.format(df.shape[1]-1)].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 2., 0.,  ..., 0., 1., 3.],\n",
      "        [2., 0., 2.,  ..., 1., 3., 2.],\n",
      "        [0., 2., 1.,  ..., 3., 2., 3.],\n",
      "        ...,\n",
      "        [3., 3., 3.,  ..., 3., 2., 0.],\n",
      "        [3., 3., 0.,  ..., 2., 0., 3.],\n",
      "        [3., 0., 3.,  ..., 0., 3., 2.]])\n",
      "tensor([3, 2, 3,  ..., 0, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.01)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "#given that we are working with 10mil data points this is essential or we would simply run out of memory on the devices. Im using 2048 in the hope there are some gains to be made with that matrix size.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "batchsize    = 131072\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    self.bnorm1 = nn.BatchNorm1d(16)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# create a class for the model WITH BATCH NORM\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,16)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.fc1    = nn.Linear(16,32)\n",
    "#\t    self.bnorm1 = nn.BatchNorm1d(16) \n",
    "\t\tself.fc2    = nn.Linear(32,32)\n",
    "#\t    self.bnorm2 = nn.BatchNorm1d(32) \n",
    "\t\tself.fc3    = nn.Linear(32,20)\n",
    "\t\t\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(20,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "#\t    x = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.fc1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "#\t    x = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.fc2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\t\n",
    "\t\tx = F.relu( self.fc3(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel():\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose the BCE loss function as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "    \n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "   \n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# NOTE: batch normalization should be turned off during testing, so we definitely need to switch modes here\n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model\n",
    "\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = df.shape[1]-1\n",
    "output_dim = 4\n",
    "numofepochs = 1000\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with minibatch size=' + str(batchsize))\n",
    "\n",
    "ax[1].plot(trainAcc,'ro-')\n",
    "ax[1].plot(testAcc,'bs-')\n",
    "ax[1].set_title('Accuracy with minibatch size=' + str(batchsize))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([27,103])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
