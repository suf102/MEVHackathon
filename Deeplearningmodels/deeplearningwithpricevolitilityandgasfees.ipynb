{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach with price volatility and Gas fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dateletswvg/datalet2.csv',index_col=False)\n",
    "df = df.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period0</th>\n",
       "      <th>period1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9998 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      period0  period1\n",
       "0           1        0\n",
       "1           0        1\n",
       "2           1        3\n",
       "3           3        2\n",
       "4           2        3\n",
       "...       ...      ...\n",
       "9993        0        0\n",
       "9994        0        1\n",
       "9995        1        0\n",
       "9996        0        0\n",
       "9997        0        1\n",
       "\n",
       "[9998 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = []\n",
    "\n",
    "for i in range(df.shape[1]-1):\n",
    "\tdataheaders.append('period{}'.format(i))\n",
    " \n",
    "\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['period{}'.format(df.shape[1]-1)].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([0, 1, 3,  ..., 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.01)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "#given that we are working with 10mil data points this is essential or we would simply run out of memory on the devices. Im using 2048 in the hope there are some gains to be made with that matrix size.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 10\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batches,shuffle=True,drop_last=True, num_workers=12)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,4)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.hidden1    = nn.Linear(4,4)\n",
    "\t\tself.bnorm1 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden2    = nn.Linear(4,4)\n",
    "\t\tself.bnorm2 = nn.BatchNorm1d(4) \n",
    "\t\tself.hidden3    = nn.Linear(4,4)\n",
    "\t\tself.bnorm3 = nn.BatchNorm1d(4)\n",
    "\t\tself.hidden4    = nn.Linear(4,4)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(4,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x)\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel():\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose cross entropy loss as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont overfit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 10.8465534 \n",
      "epoch 1 done at time 12.4781585 \n",
      "epoch 2 done at time 14.111722 \n",
      "epoch 3 done at time 15.7287169 \n",
      "epoch 4 done at time 17.316176 \n",
      "epoch 5 done at time 18.9855679 \n",
      "epoch 6 done at time 20.6600191 \n",
      "epoch 7 done at time 22.2471192 \n",
      "epoch 8 done at time 23.9650633 \n",
      "epoch 9 done at time 25.7874232 \n",
      "epoch 10 done at time 27.4530251 \n",
      "epoch 11 done at time 29.0602646 \n",
      "epoch 12 done at time 30.6723377 \n",
      "epoch 13 done at time 32.3323578 \n",
      "epoch 14 done at time 33.9239021 \n",
      "epoch 15 done at time 35.5583683 \n",
      "epoch 16 done at time 37.1484096 \n",
      "epoch 17 done at time 38.7722669 \n",
      "epoch 18 done at time 40.4143017 \n",
      "epoch 19 done at time 42.0673206 \n",
      "epoch 20 done at time 43.7340541 \n",
      "epoch 21 done at time 45.3734308 \n",
      "epoch 22 done at time 47.0090331 \n",
      "epoch 23 done at time 48.6047543 \n",
      "epoch 24 done at time 50.3447107 \n",
      "epoch 25 done at time 52.1696433 \n",
      "epoch 26 done at time 53.8996059 \n",
      "epoch 27 done at time 55.6044672 \n",
      "epoch 28 done at time 57.3765171 \n",
      "epoch 29 done at time 59.1003829 \n",
      "epoch 30 done at time 60.7114964 \n",
      "epoch 31 done at time 62.2998701 \n",
      "epoch 32 done at time 63.8904252 \n",
      "epoch 33 done at time 65.6477712 \n",
      "epoch 34 done at time 67.5107493 \n",
      "epoch 35 done at time 69.1778038 \n",
      "epoch 36 done at time 70.8170606 \n",
      "epoch 37 done at time 72.4532086 \n",
      "epoch 38 done at time 74.1566919 \n",
      "epoch 39 done at time 75.8268449 \n",
      "epoch 40 done at time 77.4761238 \n",
      "epoch 41 done at time 79.1969485 \n",
      "epoch 42 done at time 80.8568111 \n",
      "epoch 43 done at time 82.5827291 \n",
      "epoch 44 done at time 84.2731763 \n",
      "epoch 45 done at time 85.9024425 \n",
      "epoch 46 done at time 87.6037888 \n",
      "epoch 47 done at time 89.2971199 \n",
      "epoch 48 done at time 91.5418102 \n",
      "epoch 49 done at time 93.2302218 \n",
      "epoch 50 done at time 94.8587837 \n",
      "epoch 51 done at time 96.4745313 \n",
      "epoch 52 done at time 98.1686321 \n",
      "epoch 53 done at time 99.8356645 \n",
      "epoch 54 done at time 101.4851543 \n",
      "epoch 55 done at time 103.1514199 \n",
      "epoch 56 done at time 104.8443856 \n",
      "epoch 57 done at time 106.5307506 \n",
      "epoch 58 done at time 108.3197236 \n",
      "epoch 59 done at time 110.0306629 \n",
      "epoch 60 done at time 111.6734353 \n",
      "epoch 61 done at time 113.3147801 \n",
      "epoch 62 done at time 114.9291997 \n",
      "epoch 63 done at time 116.5701201 \n",
      "epoch 64 done at time 118.1766634 \n",
      "epoch 65 done at time 119.8037058 \n",
      "epoch 66 done at time 121.5939988 \n",
      "epoch 67 done at time 123.3655825 \n",
      "epoch 68 done at time 125.0325479 \n",
      "epoch 69 done at time 126.663582 \n",
      "epoch 70 done at time 128.3105508 \n",
      "epoch 71 done at time 129.9682023 \n",
      "epoch 72 done at time 131.6230084 \n",
      "epoch 73 done at time 133.298658 \n",
      "epoch 74 done at time 134.9061629 \n",
      "epoch 75 done at time 136.5392062 \n",
      "epoch 76 done at time 138.2691227 \n",
      "epoch 77 done at time 139.9915256 \n",
      "epoch 78 done at time 141.6172823 \n",
      "epoch 79 done at time 143.312673 \n",
      "epoch 80 done at time 144.9623401 \n",
      "epoch 81 done at time 146.6421075 \n",
      "epoch 82 done at time 148.2665605 \n",
      "epoch 83 done at time 149.933975 \n",
      "epoch 84 done at time 151.5789126 \n",
      "epoch 85 done at time 153.2289246 \n",
      "epoch 86 done at time 154.8433197 \n",
      "epoch 87 done at time 156.6578136 \n",
      "epoch 88 done at time 158.3081253 \n",
      "epoch 89 done at time 159.9917824 \n",
      "epoch 90 done at time 161.8021425 \n",
      "epoch 91 done at time 163.610827 \n",
      "epoch 92 done at time 165.2671945 \n",
      "epoch 93 done at time 166.9539962 \n",
      "epoch 94 done at time 168.6580306 \n",
      "epoch 95 done at time 170.3110577 \n",
      "epoch 96 done at time 172.0125542 \n",
      "epoch 97 done at time 173.6534863 \n",
      "epoch 98 done at time 175.3605945 \n",
      "epoch 99 done at time 177.0634025 \n",
      "epoch 100 done at time 178.7748912 \n",
      "epoch 101 done at time 180.4676552 \n",
      "epoch 102 done at time 182.128058 \n",
      "epoch 103 done at time 183.7780808 \n",
      "epoch 104 done at time 185.4039305 \n",
      "epoch 105 done at time 187.0441807 \n",
      "epoch 106 done at time 188.8353027 \n",
      "epoch 107 done at time 190.5051669 \n",
      "epoch 108 done at time 192.1693879 \n",
      "epoch 109 done at time 193.8035053 \n",
      "epoch 110 done at time 195.4417553 \n",
      "epoch 111 done at time 197.1901314 \n",
      "epoch 112 done at time 198.8428726 \n",
      "epoch 113 done at time 200.547645 \n",
      "epoch 114 done at time 202.2420832 \n",
      "epoch 115 done at time 203.9180607 \n",
      "epoch 116 done at time 205.5751624 \n",
      "epoch 117 done at time 207.285397 \n"
     ]
    }
   ],
   "source": [
    "input_dim = df.shape[1]-1\n",
    "output_dim = 4\n",
    "numofepochs = 1000\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epoch')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Accuracy epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
