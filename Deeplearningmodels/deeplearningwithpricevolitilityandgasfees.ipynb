{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach with price volatility and Gas fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Count the number of CPU cores available, this returns the number of threads, because most cpus have a thread count equal to twice their core count I have halved the count when multithreading\n",
    "# Set this number to one if you dont want to multi thread the process\n",
    "\n",
    "cpuCount = os.cpu_count()\n",
    "print(cpuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the data from the CSV\n",
    "\n",
    "#This value will allow you to change how many periods back the data will consider. for example if you select 1 it will only look at the previous period, Maximum value is currently set to 8.\n",
    "#if you want to try over more data periods a greater number of datalet length will need to be prepped in the data prep folders. By default a maximum of 8\n",
    "\n",
    "dataperiods = 699\n",
    "\n",
    "df = pd.read_csv('../dataletswgv/datalet{}.csv'.format(dataperiods+1),index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mev_period0</th>\n",
       "      <th>Mev_period1</th>\n",
       "      <th>Mev_period2</th>\n",
       "      <th>Mev_period3</th>\n",
       "      <th>Mev_period4</th>\n",
       "      <th>Mev_period5</th>\n",
       "      <th>Mev_period6</th>\n",
       "      <th>Mev_period7</th>\n",
       "      <th>Mev_period8</th>\n",
       "      <th>Mev_period9</th>\n",
       "      <th>...</th>\n",
       "      <th>price_volitility_period690</th>\n",
       "      <th>price_volitility_period691</th>\n",
       "      <th>price_volitility_period692</th>\n",
       "      <th>price_volitility_period693</th>\n",
       "      <th>price_volitility_period694</th>\n",
       "      <th>price_volitility_period695</th>\n",
       "      <th>price_volitility_period696</th>\n",
       "      <th>price_volitility_period697</th>\n",
       "      <th>price_volitility_period698</th>\n",
       "      <th>price_volitility_period699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.004328e+08</td>\n",
       "      <td>3.158093e+08</td>\n",
       "      <td>3.296369e+08</td>\n",
       "      <td>3.421034e+08</td>\n",
       "      <td>3.533529e+08</td>\n",
       "      <td>3.634985e+08</td>\n",
       "      <td>3.726302e+08</td>\n",
       "      <td>3.808210e+08</td>\n",
       "      <td>3.881306e+08</td>\n",
       "      <td>3.946078e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.158093e+08</td>\n",
       "      <td>3.296369e+08</td>\n",
       "      <td>3.421034e+08</td>\n",
       "      <td>3.533529e+08</td>\n",
       "      <td>3.634985e+08</td>\n",
       "      <td>3.726302e+08</td>\n",
       "      <td>3.808210e+08</td>\n",
       "      <td>3.881306e+08</td>\n",
       "      <td>3.946078e+08</td>\n",
       "      <td>4.002932e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.296369e+08</td>\n",
       "      <td>3.421034e+08</td>\n",
       "      <td>3.533529e+08</td>\n",
       "      <td>3.634985e+08</td>\n",
       "      <td>3.726302e+08</td>\n",
       "      <td>3.808210e+08</td>\n",
       "      <td>3.881306e+08</td>\n",
       "      <td>3.946078e+08</td>\n",
       "      <td>4.002932e+08</td>\n",
       "      <td>4.052199e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.421034e+08</td>\n",
       "      <td>3.533529e+08</td>\n",
       "      <td>3.634985e+08</td>\n",
       "      <td>3.726302e+08</td>\n",
       "      <td>3.808210e+08</td>\n",
       "      <td>3.881306e+08</td>\n",
       "      <td>3.946078e+08</td>\n",
       "      <td>4.002932e+08</td>\n",
       "      <td>4.052199e+08</td>\n",
       "      <td>4.094155e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.533529e+08</td>\n",
       "      <td>3.634985e+08</td>\n",
       "      <td>3.726302e+08</td>\n",
       "      <td>3.808210e+08</td>\n",
       "      <td>3.881306e+08</td>\n",
       "      <td>3.946078e+08</td>\n",
       "      <td>4.002932e+08</td>\n",
       "      <td>4.052199e+08</td>\n",
       "      <td>4.094155e+08</td>\n",
       "      <td>4.129021e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.613917e+08</td>\n",
       "      <td>3.611025e+08</td>\n",
       "      <td>3.602334e+08</td>\n",
       "      <td>3.587803e+08</td>\n",
       "      <td>3.567359e+08</td>\n",
       "      <td>3.540901e+08</td>\n",
       "      <td>3.508293e+08</td>\n",
       "      <td>3.469361e+08</td>\n",
       "      <td>3.423889e+08</td>\n",
       "      <td>3.371612e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.611025e+08</td>\n",
       "      <td>3.602334e+08</td>\n",
       "      <td>3.587803e+08</td>\n",
       "      <td>3.567359e+08</td>\n",
       "      <td>3.540901e+08</td>\n",
       "      <td>3.508293e+08</td>\n",
       "      <td>3.469361e+08</td>\n",
       "      <td>3.423889e+08</td>\n",
       "      <td>3.371612e+08</td>\n",
       "      <td>3.312210e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.602334e+08</td>\n",
       "      <td>3.587803e+08</td>\n",
       "      <td>3.567359e+08</td>\n",
       "      <td>3.540901e+08</td>\n",
       "      <td>3.508293e+08</td>\n",
       "      <td>3.469361e+08</td>\n",
       "      <td>3.423889e+08</td>\n",
       "      <td>3.371612e+08</td>\n",
       "      <td>3.312210e+08</td>\n",
       "      <td>3.232888e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.587803e+08</td>\n",
       "      <td>3.567359e+08</td>\n",
       "      <td>3.540901e+08</td>\n",
       "      <td>3.508293e+08</td>\n",
       "      <td>3.469361e+08</td>\n",
       "      <td>3.423889e+08</td>\n",
       "      <td>3.371612e+08</td>\n",
       "      <td>3.312210e+08</td>\n",
       "      <td>3.232888e+08</td>\n",
       "      <td>3.147701e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.567359e+08</td>\n",
       "      <td>3.540901e+08</td>\n",
       "      <td>3.508293e+08</td>\n",
       "      <td>3.469361e+08</td>\n",
       "      <td>3.423889e+08</td>\n",
       "      <td>3.371612e+08</td>\n",
       "      <td>3.312210e+08</td>\n",
       "      <td>3.232888e+08</td>\n",
       "      <td>3.147701e+08</td>\n",
       "      <td>3.056032e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4457 rows × 2100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Mev_period0  Mev_period1  Mev_period2  Mev_period3  Mev_period4  \\\n",
       "0             0.0          0.0          0.0          0.0          0.0   \n",
       "1             0.0          0.0          0.0          0.0          0.0   \n",
       "2             0.0          0.0          0.0          0.0          0.0   \n",
       "3             0.0          0.0          0.0          0.0          0.0   \n",
       "4             0.0          0.0          0.0          0.0          0.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "4452          0.0          0.0          0.0          0.0          0.0   \n",
       "4453          0.0          0.0          0.0          0.0          0.0   \n",
       "4454          0.0          0.0          0.0          0.0          0.0   \n",
       "4455          0.0          0.0          0.0          0.0          0.0   \n",
       "4456          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "      Mev_period5  Mev_period6  Mev_period7  Mev_period8  Mev_period9  ...  \\\n",
       "0             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "4452          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4453          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4454          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4455          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4456          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "      price_volitility_period690  price_volitility_period691  \\\n",
       "0                   3.004328e+08                3.158093e+08   \n",
       "1                   3.158093e+08                3.296369e+08   \n",
       "2                   3.296369e+08                3.421034e+08   \n",
       "3                   3.421034e+08                3.533529e+08   \n",
       "4                   3.533529e+08                3.634985e+08   \n",
       "...                          ...                         ...   \n",
       "4452                3.613917e+08                3.611025e+08   \n",
       "4453                3.611025e+08                3.602334e+08   \n",
       "4454                3.602334e+08                3.587803e+08   \n",
       "4455                3.587803e+08                3.567359e+08   \n",
       "4456                3.567359e+08                3.540901e+08   \n",
       "\n",
       "      price_volitility_period692  price_volitility_period693  \\\n",
       "0                   3.296369e+08                3.421034e+08   \n",
       "1                   3.421034e+08                3.533529e+08   \n",
       "2                   3.533529e+08                3.634985e+08   \n",
       "3                   3.634985e+08                3.726302e+08   \n",
       "4                   3.726302e+08                3.808210e+08   \n",
       "...                          ...                         ...   \n",
       "4452                3.602334e+08                3.587803e+08   \n",
       "4453                3.587803e+08                3.567359e+08   \n",
       "4454                3.567359e+08                3.540901e+08   \n",
       "4455                3.540901e+08                3.508293e+08   \n",
       "4456                3.508293e+08                3.469361e+08   \n",
       "\n",
       "      price_volitility_period694  price_volitility_period695  \\\n",
       "0                   3.533529e+08                3.634985e+08   \n",
       "1                   3.634985e+08                3.726302e+08   \n",
       "2                   3.726302e+08                3.808210e+08   \n",
       "3                   3.808210e+08                3.881306e+08   \n",
       "4                   3.881306e+08                3.946078e+08   \n",
       "...                          ...                         ...   \n",
       "4452                3.567359e+08                3.540901e+08   \n",
       "4453                3.540901e+08                3.508293e+08   \n",
       "4454                3.508293e+08                3.469361e+08   \n",
       "4455                3.469361e+08                3.423889e+08   \n",
       "4456                3.423889e+08                3.371612e+08   \n",
       "\n",
       "      price_volitility_period696  price_volitility_period697  \\\n",
       "0                   3.726302e+08                3.808210e+08   \n",
       "1                   3.808210e+08                3.881306e+08   \n",
       "2                   3.881306e+08                3.946078e+08   \n",
       "3                   3.946078e+08                4.002932e+08   \n",
       "4                   4.002932e+08                4.052199e+08   \n",
       "...                          ...                         ...   \n",
       "4452                3.508293e+08                3.469361e+08   \n",
       "4453                3.469361e+08                3.423889e+08   \n",
       "4454                3.423889e+08                3.371612e+08   \n",
       "4455                3.371612e+08                3.312210e+08   \n",
       "4456                3.312210e+08                3.232888e+08   \n",
       "\n",
       "      price_volitility_period698  price_volitility_period699  \n",
       "0                   3.881306e+08                3.946078e+08  \n",
       "1                   3.946078e+08                4.002932e+08  \n",
       "2                   4.002932e+08                4.052199e+08  \n",
       "3                   4.052199e+08                4.094155e+08  \n",
       "4                   4.094155e+08                4.129021e+08  \n",
       "...                          ...                         ...  \n",
       "4452                3.423889e+08                3.371612e+08  \n",
       "4453                3.371612e+08                3.312210e+08  \n",
       "4454                3.312210e+08                3.232888e+08  \n",
       "4455                3.232888e+08                3.147701e+08  \n",
       "4456                3.147701e+08                3.056032e+08  \n",
       "\n",
       "[4457 rows x 2100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = list(df.columns)\n",
    "dataheaders.remove('Mev_period{}'.format(int((df.shape[1]/3-1))))\n",
    "dataheaders.remove('gas_fees_period{}'.format(int((df.shape[1]/3-1))))\n",
    "dataheaders.remove('price_volitility_period{}'.format(int((df.shape[1]/3-1))))\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['Mev_period{}'.format(int((df.shape[1]/3)-1))].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7263e+08, 3.8082e+08,\n",
      "         3.8813e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8082e+08, 3.8813e+08,\n",
      "         3.9461e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8813e+08, 3.9461e+08,\n",
      "         4.0029e+08],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4239e+08, 3.3716e+08,\n",
      "         3.3122e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3716e+08, 3.3122e+08,\n",
      "         3.2329e+08],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3122e+08, 3.2329e+08,\n",
      "         3.1477e+08]])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.011)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 11\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batches,shuffle=True,drop_last=True, num_workers=(int(cpuCount/2)))\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,64)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.bnorm1 \t= nn.BatchNorm1d(64) \n",
    "\t\tself.hidden1    = nn.Linear(64,256)\n",
    "\t\tself.bnorm2 \t= nn.BatchNorm1d(256) \n",
    "\t\tself.hidden2    = nn.Linear(256,256)\n",
    "\t\tself.bnorm3 \t= nn.BatchNorm1d(256)\n",
    "\t\tself.hidden3    = nn.Linear(256,256)\n",
    "\t\tself.bnorm4 \t= nn.BatchNorm1d(256)\n",
    "\t\tself.hidden4    = nn.Linear(256,256)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(256,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x)\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\t\tx = self.bnorm4(x)\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel(learning):\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose cross entropy loss as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=learning)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont overfit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 4543.6128651 \n",
      "epoch 1 done at time 4548.6548408 \n",
      "epoch 2 done at time 4552.0920931 \n",
      "epoch 3 done at time 4555.7544985 \n",
      "epoch 4 done at time 4557.8962295 \n",
      "epoch 5 done at time 4562.854604 \n",
      "epoch 6 done at time 4566.4881146 \n",
      "epoch 7 done at time 4568.7156491 \n",
      "epoch 8 done at time 4573.3351702 \n",
      "epoch 9 done at time 4576.6606626 \n",
      "epoch 10 done at time 4578.8365926 \n",
      "epoch 11 done at time 4581.3695864 \n",
      "epoch 12 done at time 4585.5691721 \n",
      "epoch 13 done at time 4589.0700079 \n",
      "epoch 14 done at time 4592.7469827 \n",
      "epoch 15 done at time 4594.8949911 \n",
      "epoch 16 done at time 4598.2030871 \n",
      "epoch 17 done at time 4600.3786771 \n",
      "epoch 18 done at time 4604.9871432 \n",
      "epoch 19 done at time 4609.8767402 \n",
      "epoch 20 done at time 4613.2620247 \n",
      "epoch 21 done at time 4616.8359648 \n",
      "epoch 22 done at time 4618.9719845 \n",
      "epoch 23 done at time 4623.6042413 \n",
      "epoch 24 done at time 4628.3686665 \n",
      "epoch 25 done at time 4632.0233996 \n",
      "epoch 26 done at time 4635.872918 \n",
      "epoch 27 done at time 4639.0737727 \n",
      "epoch 28 done at time 4641.2821532 \n",
      "epoch 29 done at time 4645.2231223 \n",
      "epoch 30 done at time 4647.3542193 \n",
      "epoch 31 done at time 4651.6880954 \n",
      "epoch 32 done at time 4653.889313 \n",
      "epoch 33 done at time 4658.8560046 \n",
      "epoch 34 done at time 4661.9954486 \n",
      "epoch 35 done at time 4664.1468052 \n",
      "epoch 36 done at time 4667.4530295 \n",
      "epoch 37 done at time 4670.9047397 \n",
      "epoch 38 done at time 4674.4375718 \n",
      "epoch 39 done at time 4676.6117519 \n",
      "epoch 40 done at time 4678.7365919 \n",
      "epoch 41 done at time 4682.9574725 \n",
      "epoch 42 done at time 4686.5376834 \n",
      "epoch 43 done at time 4690.0045697 \n",
      "epoch 44 done at time 4693.5435804 \n",
      "epoch 45 done at time 4697.2076232 \n",
      "epoch 46 done at time 4699.3311894 \n",
      "epoch 47 done at time 4703.9217852 \n",
      "epoch 48 done at time 4707.9452298 \n",
      "epoch 49 done at time 4712.5221642 \n",
      "epoch 50 done at time 4715.8806642 \n",
      "epoch 51 done at time 4719.6893176 \n",
      "epoch 52 done at time 4723.1369549 \n",
      "epoch 53 done at time 4725.8934212 \n",
      "epoch 54 done at time 4730.6941213 \n",
      "epoch 55 done at time 4734.2429952 \n",
      "epoch 56 done at time 4736.5060878 \n",
      "epoch 57 done at time 4741.3207725 \n",
      "epoch 58 done at time 4745.6242833 \n",
      "epoch 59 done at time 4748.7859096 \n",
      "epoch 60 done at time 4752.3057966 \n",
      "epoch 61 done at time 4756.0321487 \n",
      "epoch 62 done at time 4759.3458511 \n",
      "epoch 63 done at time 4761.4744785 \n",
      "epoch 64 done at time 4766.1001366 \n",
      "epoch 65 done at time 4770.6694822 \n",
      "epoch 66 done at time 4774.2539791 \n",
      "epoch 67 done at time 4776.3410562 \n",
      "epoch 68 done at time 4780.7946437 \n",
      "epoch 69 done at time 4784.3915372 \n",
      "epoch 70 done at time 4788.1049873 \n",
      "epoch 71 done at time 4791.3480015 \n",
      "epoch 72 done at time 4795.2542559 \n",
      "epoch 73 done at time 4798.6524626 \n",
      "epoch 74 done at time 4800.7296684 \n",
      "epoch 75 done at time 4803.3593508 \n",
      "epoch 76 done at time 4805.4908491 \n",
      "epoch 77 done at time 4809.9584463 \n",
      "epoch 78 done at time 4813.574494 \n",
      "epoch 79 done at time 4817.4742894 \n",
      "epoch 80 done at time 4819.6160893 \n",
      "epoch 81 done at time 4822.090912 \n",
      "epoch 82 done at time 4826.7344932 \n",
      "epoch 83 done at time 4830.038385 \n",
      "epoch 84 done at time 4833.525478 \n",
      "epoch 85 done at time 4837.3277947 \n",
      "epoch 86 done at time 4841.8389511 \n",
      "epoch 87 done at time 4845.2281409 \n",
      "epoch 88 done at time 4847.3776787 \n",
      "epoch 89 done at time 4851.704914 \n",
      "epoch 90 done at time 4855.0697606 \n",
      "epoch 91 done at time 4858.6287338 \n",
      "epoch 92 done at time 4862.19092 \n",
      "epoch 93 done at time 4865.3054638 \n",
      "epoch 94 done at time 4868.6724385 \n",
      "epoch 95 done at time 4872.3152148 \n",
      "epoch 96 done at time 4874.4592693 \n",
      "epoch 97 done at time 4878.9201909 \n",
      "epoch 98 done at time 4881.0634447 \n",
      "epoch 99 done at time 4886.4722137 \n",
      "epoch 100 done at time 4889.775573 \n",
      "epoch 101 done at time 4893.795995 \n",
      "epoch 102 done at time 4896.0073497 \n",
      "epoch 103 done at time 4900.7379654 \n",
      "epoch 104 done at time 4904.2131301 \n",
      "epoch 105 done at time 4906.4313468 \n",
      "epoch 106 done at time 4911.194138 \n",
      "epoch 107 done at time 4914.9697039 \n",
      "epoch 108 done at time 4918.6158041 \n",
      "epoch 109 done at time 4920.7402758 \n",
      "epoch 110 done at time 4925.4078982 \n",
      "epoch 111 done at time 4928.8785624 \n",
      "epoch 112 done at time 4932.4733855 \n",
      "epoch 113 done at time 4936.3217038 \n",
      "epoch 114 done at time 4939.040982 \n",
      "epoch 115 done at time 4944.2390099 \n",
      "epoch 116 done at time 4947.660245 \n",
      "epoch 117 done at time 4951.3639012 \n",
      "epoch 118 done at time 4955.3631344 \n",
      "epoch 119 done at time 4957.5288132 \n",
      "epoch 120 done at time 4962.3393523 \n",
      "epoch 121 done at time 4966.1730059 \n",
      "epoch 122 done at time 4968.2917441 \n",
      "epoch 123 done at time 4971.7538428 \n",
      "epoch 124 done at time 4976.005928 \n",
      "epoch 125 done at time 4978.1109585 \n",
      "epoch 126 done at time 4982.8295671 \n",
      "epoch 127 done at time 4986.6592487 \n",
      "epoch 128 done at time 4989.5806175 \n",
      "epoch 129 done at time 4991.6866608 \n",
      "epoch 130 done at time 4996.2073391 \n",
      "epoch 131 done at time 4999.7540436 \n",
      "epoch 132 done at time 5003.2721486 \n",
      "epoch 133 done at time 5006.7985785 \n",
      "epoch 134 done at time 5010.2733327 \n",
      "epoch 135 done at time 5013.1814028 \n",
      "epoch 136 done at time 5015.2943501 \n",
      "epoch 137 done at time 5020.3002101 \n",
      "epoch 138 done at time 5023.6481616 \n",
      "epoch 139 done at time 5027.3968963 \n",
      "epoch 140 done at time 5031.5687654 \n",
      "epoch 141 done at time 5035.1093245 \n",
      "epoch 142 done at time 5038.4891658 \n",
      "epoch 143 done at time 5040.6280525 \n",
      "epoch 144 done at time 5045.6544436 \n",
      "epoch 145 done at time 5049.8690781 \n",
      "epoch 146 done at time 5053.8049429 \n",
      "epoch 147 done at time 5057.540571 \n",
      "epoch 148 done at time 5061.0533818 \n",
      "epoch 149 done at time 5063.2134342 \n",
      "epoch 150 done at time 5068.2822007 \n",
      "epoch 151 done at time 5071.7903813 \n",
      "epoch 152 done at time 5075.6020867 \n",
      "epoch 153 done at time 5077.6937206 \n",
      "epoch 154 done at time 5082.8172504 \n",
      "epoch 155 done at time 5087.9844534 \n",
      "epoch 156 done at time 5094.5015415 \n",
      "epoch 157 done at time 5099.91564 \n",
      "epoch 158 done at time 5104.4055621 \n",
      "epoch 159 done at time 5108.0245761 \n",
      "epoch 160 done at time 5110.2410282 \n",
      "epoch 161 done at time 5115.1225393 \n",
      "epoch 162 done at time 5117.9811747 \n",
      "epoch 163 done at time 5121.0140437 \n",
      "epoch 164 done at time 5123.1084781 \n",
      "epoch 165 done at time 5126.5595417 \n",
      "epoch 166 done at time 5130.6805189 \n",
      "epoch 167 done at time 5134.1259101 \n",
      "epoch 168 done at time 5138.1483832 \n",
      "epoch 169 done at time 5140.3839961 \n",
      "epoch 170 done at time 5145.092181 \n",
      "epoch 171 done at time 5148.7193843 \n",
      "epoch 172 done at time 5150.8474892 \n",
      "epoch 173 done at time 5154.5952016 \n",
      "epoch 174 done at time 5158.1291015 \n",
      "epoch 175 done at time 5160.1841462 \n",
      "epoch 176 done at time 5163.7461599 \n",
      "epoch 177 done at time 5165.8443486 \n",
      "epoch 178 done at time 5170.7479183 \n",
      "epoch 179 done at time 5173.6604925 \n",
      "epoch 180 done at time 5175.8168176 \n",
      "epoch 181 done at time 5179.2736678 \n",
      "epoch 182 done at time 5181.4559827 \n",
      "epoch 183 done at time 5187.47171 \n",
      "epoch 184 done at time 5191.5475565 \n",
      "epoch 185 done at time 5195.4712129 \n",
      "epoch 186 done at time 5199.5601288 \n",
      "epoch 187 done at time 5202.7729496 \n",
      "epoch 188 done at time 5206.5738999 \n",
      "epoch 189 done at time 5208.9403566 \n",
      "epoch 190 done at time 5212.5463647 \n",
      "epoch 191 done at time 5214.7722321 \n",
      "epoch 192 done at time 5219.8762297 \n",
      "epoch 193 done at time 5223.7948757 \n",
      "epoch 194 done at time 5228.0395456 \n",
      "epoch 195 done at time 5232.3446947 \n",
      "epoch 196 done at time 5234.7491012 \n",
      "epoch 197 done at time 5238.7886446 \n",
      "epoch 198 done at time 5241.102508 \n",
      "epoch 199 done at time 5245.8099374 \n"
     ]
    }
   ],
   "source": [
    "# Time to run the model, first we need to input parameters, you might want to change the number of epochs if it isn't reaching the level of accuracy desired.\n",
    "\n",
    "input_dim = df.shape[1]-3\n",
    "output_dim = 5\n",
    "numofepochs = 200\n",
    "learningrate = 0.01\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel(learningrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAFNCAYAAABSRs15AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABH20lEQVR4nO3de5xcdX34/9d7ZzfZXAmXSALhKqBglYCpt1ZE7UWsCr1YsWqptaWordrWCtqvSn9rf1X7a7/VilWqRAUV7xZbrFSqRU1QgwsKAgUFIWYD4ZIL5LI7M+/fH3N2mWx2N5fd2Zmz+3o+HuexM+ecOee9ZyfzyXven/P5RGYiSZIkSSq/rnYHIEmSJEmaGiZ4kiRJkjRDmOBJkiRJ0gxhgidJkiRJM4QJniRJkiTNECZ4kiRJkjRDmOBJmhYRcXFEXNHuOCRJajXbPLWTCZ5mlIi4OyJ+pd1xSJJUFhHxzYh4OCLmtjsWSZNngifNcBHR3e4YJEmdKSKOBZ4NJPCSaT637ZPUAiZ4mhUiYm5E/FNEbCiWfxr+pjIiDouIf4+IzRHxUER8KyK6im0XRsTPI2JbRNweEc8v1ndFxEUR8ZOIeDAiPhsRhxTbeiPiimL95oj4fkQcPk5cJxffnG6OiFsi4iXF+mdExMaIqDTt+5sR8cN9OP+xEZER8ZqIuAf473HO/aKIuLE495qIeErTtrsj4q0R8ePiW93VEdHbtP2PI+LO4npdFRFHNG17UkT8V7Htvoh4W9Np50TEJ4rreUtErNq/v6QkaYr9PnA98DHgvOYNEXFURHwxIjYVbc0Hmrb9cUTcWnye/zgiTi/WZ0Sc0LTfxyLiXcXjMyNifdG2bgRWR8TBRRu8qWhv/j0iVjS9/pCiDdpQbP9ysf7miHhx0349EfFARKwc65fstDZvvP9fSFPBBE+zxV8DzwBWAqcCTwP+T7HtL4H1wFLgcOBtQEbEE4A/BX4xMxcBvw7cXbzmDcA5wHOAI4CHgUuKbecBBwFHAYcCFwA7RgcUET3AV4BrgMcBfwZ8MiKekJnXA48Cz2t6ye8Bn9qH8w97DnByEffoc58OXAb8SRHjh4GrYvfuOa8oXvt44KTh6xURzwP+DvhdYDnwM+DKYtsi4OvAfxZxnQBc23TMlxT7LgGuAj6AJKmdfh/4ZLH8+vAXksUXjP9O4zP+WOBIHvusfylwcfHaxTQ+2x/cx/MtAw4BjgHOp/F/0dXF86NptJfNbcPlwHzgSTTayv9brP8E8Mqm/V4IDGTmjaNP2Glt3l7+fyFNXma6uMyYhcYH5K+Msf4nwAubnv86cHfx+P8B/g04YdRrTgDuB34F6Bm17Vbg+U3PlwNDQDfwh8Aa4Cl7ifXZwEagq2ndp4GLi8fvAi4rHi+ikfAdsw/nP5ZGV5vjJzj3vwB9o9bdDjyn6Tpe0LTthcBPiscfBd7btG1hce5jgZcD/eOc82Lg603PTwF2tPs94+Li4jJbF+CXi8/vw4rntwF/Xjx+JrAJ6B7jdV8D3jjOMbO5PaVRGXxX8fhMYBDonSCmlcDDxePlQB04eIz9jgC2AYuL558H3jLOMTuqzWOC/1+4uEzFYgVPs8URNL51G/azYh3A3wN3AtdExE8j4iKAzLwTeBOND+n7I+LKpm4ZxwBfKrp6bKaRcNVoVAAvp9H4XVl0KXlvUa0bK6Z7M7M+Kq4ji8efAn6r+Ibxt4AfZObw7zDR+YfdO8H1OAb4y+HXF8c4qumajH598/Xa7Vpm5iM0vrk9sjjGTyY478amx9uB3vAeDElql/OAazLzgeL5p3ism+ZRwM8yszrG6/b2WT+RTZm5c/hJRMyPiA9HxM8iYitwHbCkqCAeBTyUmQ+PPkhmbgC+A/x2RCwBzqJRhRxLR7V5e/n/hTRpJniaLTbQ+IAfdnSxjszclpl/mZnHAy8G/mK4L3xmfiozf7l4bQLvKV5/L3BWZi5pWnoz8+eZOZSZf5OZpwDPAl5EoxvLWDEdFcX9fk1x/bw4949pNCpnsXv3zAnP37RPTnA97gX+dtTr52fmp5v2OWqs68WoaxkRC2h0efl5cdzHT3BeSVIHiIh5NLodPica93xvBP4cODUiTqXxeX70OF/CTfRZv51Gl8phy0ZtH902/SXwBODpmbkYOGM4xOI8hxQJ3Fg+TqOb5kuBtaPawNHxdlSbN8H/L6RJM8HTTNQTjYFOhpduGl0f/09ELI2Iw4B3AFfAyI3XJ0REAFtpVMJqEfGEiHheUUHbSeO+gFpxjg8BfxsRxxTHWBoRZxePnxsRTy6+fdxKoytHjT19l0a3y7cUN4efSSPBvLJpn0/RuN/uDOBzTevHPf8++lfggoh4ejQsiIjfKO4nGPb6iFgRjcFb3gZ8pimmV0fEyuLa/L/AdzPzbhr3ayyLiDdFY2CbRRHx9P2IS5I0Pc6h0TadQqNb5Eoa921/i8aXkt8DBoB3F21Eb0T8UvHajwBvjoinFm3ICcPtEXAj8HsRUYmIF9C4H3wii2i0r5uL9uadwxsycwD4KvDBaAzG0hMRZzS99svA6cAbadyTN56OavP28v8LadJM8DQTXU3jw3J4uZjG/WzrgB8CPwJ+UKwDOJHGTdKPAGuBD2bmN4G5wLuBB2h0s3gcjQ99gPfRuGH6mojYRmMEsuEP9WU07gXYSqPr5P9QJJPNMnOQxg3YZxXn+CDw+5l5W9Nun6Zxz8J/N3Wh2dv59yoz1wF/TOOG74dpdFH9g1G7fYrGADA/LZZ3Fa+9Fng78AUajf/jgXOLbduAX6WRqG4E7gCeu69xSZKmzXnA6sy8JzM3Di802oVX0KigvZjG/WL30BiM7GUAmfk54G9ptBPbaCRahxTHfWPxus3Fcb68lzj+CZhHox28nsaAJc1eReOL0tto3Lf2puENmbmDRlt0HPDF8U7QgW3eRP+/kCYtMifqxSVpNoqIu4E/ysyvtzsWSZLGExHvAE7KzFfudefxj3E3tnmaQRzcQJIkSaVTdKd8DY0qn6SCXTQlSZJUKhHxxzQGOflqZl7X7nikTmIXTUmSJEmaIazgSZIkSdIMYYInSZIkSTNE6QZZOeyww/LYY49tdxiSpGlwww03PJCZS9sdR1nYRkrS7DBR+1i6BO/YY49l3bp17Q5DkjQNIuJn7Y6hTGwjJWl2mKh9tIumJEmSJM0QJniSJEmSNEOY4EmSNA0i4rKIuD8ibm5ad0hE/FdE3FH8PLhp21sj4s6IuD0ifr09UUuSysYET5Kk6fEx4AWj1l0EXJuZJwLXFs+JiFOAc4EnFa/5YERUpi9USVJZmeBJkjQNMvM64KFRq88GPl48/jhwTtP6KzNzV2beBdwJPG064pQklVvpRtGUJGkGOTwzBwAycyAiHlesPxK4vmm/9cW61vrqRbDxRy0/jSTNasueDGe9u2WHt4InSVLniTHW5Zg7RpwfEesiYt2mTZtaHJYkqdPNugrewMAA5557Lp/5zGdYtmxZu8ORJM1u90XE8qJ6txy4v1i/Hjiqab8VwIaxDpCZlwKXAqxatWrMJHCfjfGN8v1bd/Kjn29h8bwetmwf4vb7tlHpChb39tBdCboiqHRBV8TIUumCSlcX3ZWguyvYuqPK/dt2AtBT6WJOpYs53V30VLqo1uvsqtaZU2nsv2uoTj2T3p4KW3YMsWnbLrq7goW93Sw/qJehWnLPQ9s5dMEcjl+6kM3bB9m2s8qCud0s6u2mt6fCvQ9t52cPbmfenC4W9/aweF4Pi3q7WTC3m83bB3lg2yA5Kl8OgkW93cyb0zjvzqHaHteip9LFssW9zOnuYvP2Iar1OrU6bNkxxPbBKpWuoLvSRXdX0DVWij6GnkoXS+b3UK0lW3YM0VNpXJedQ7WR67BzqMbWnVUWzq0U171xrXq6upjb00VXBOsf3s6mbbsaMRRxdEXj7/K4xb0snNvNlh2DDFbrY8aRCY8O1ti6Y4gl83s4bOFc6plUa0m1Xueeh7bzk/sfZVFvN0vm97B1Z5XengqnHbWE3p4KW3cOMafSRZJs3LKLHUM15vVU6O3potIVPPDIIDuHahx9yHwWzu1m264qj+yssmOoxqLebubPqbBzqE5PJThoXg9DtWTbziEe2VUFYNniXub2VBis1htLrTbyeFe1TqUrivNVRt5HCfT2dFGrJ4PV+sj7Y8uOIYZqSW9PF9t31di2q8qi3m4WzW38t3j7YI0tO4Y4aF4PBy/o4YFtgzw6WGXJ/J6Rv3dX7Ple3jFUY/P2QSKCnkpQ6QoqEcQE74VMeGRXla07hqh0ddHTHbsdc6z30aLeHh63aC7bB2ts2raLTY/sYrBaZ8n8Hg6a18OS+XPo6QqG6snm7YPUM1kybw6PDla5f+suuivBgjndLDuol0pXcN/WnXRFsHheDzuHamzbWeWRnUP0dHdx+KJeHh2ssmnbLoDG79TV+HfeUwkOXTCXQxbOYdvOKkPVOgcv6KErYuRvGRFs2raTbcX7ZVe1XrzPG7/Yxi076al0sfKoJVTryb0PbadWf+zfZk8lWNjbw8K5jffIUK1Otd74d5GZ7Byqs3OoNrJ0dQWHL+5l0dxu6snIe2jB3G4Wzu1mYW8323fVGr9zF8zrqTC3pzLy3tm6Y4j7i8+crq5gy/bBxudTd+Nv3VPpoqe7i56uvf9dh99HleLz4P5tu3h0V42D5/fwpCMX87wJPhcma9YleH19fXz729+mr6+PSy65pN3hSJJmt6uA84B3Fz//rWn9pyLiH4EjgBOB701nYI/sqnLhF37I127eSLU+ubxRM8NhC+fwyK4qO4fqzO3uYqhWx7eGJqO7K6hnzor3UVdAb0+F7YM1zl55BM974uEtO9esSvAGBga47LLLqNfrXHbZZbz97W+3iidJmhYR8WngTOCwiFgPvJNGYvfZiHgNcA/wUoDMvCUiPgv8GKgCr8/MPUtKLXTTvZv5jx8O8LurVvDSVUexY7DGgrkVnrBsMV3RqGLU6km9DvVMaplkJrU61OpJrd6o/FTrycK53Ry+uJeugMFao+oyVGtUVborjYrFyDfz3Y3BQndWayzu7WHpokYlaeuOIQa27KTSFRxz6Hw2bdvFXQ88yqEL5rJ4XjePFBWhRwerHLlkPscdtoDBWp1tO4fYuqPK1p1DPLKzykHze1i6cC6VUaWRxjmq7BiqctC8HubN6d6jn+yuap2BLTuo1pIl83uKCkuweF6jOlirJdXi9859/A/rYLXOw9sHqXQ1Kle1eo5Ul7oi2DFUo7e7MvI7bt1RbexTr1OtJbuqNYZqdY5cMp9lB/WS2Xj9cAzVWnL/tp1s21VlybweenvGH4x1/pwKi3p72Lx9kAcfHRyp0lS6guUHzeOQBXPITHZV6/T2VHh0V5Uf/XwL9XqyeF4PQ7VG1WzZ4t6RitzOoUZ8hy2cS29PhbsffJQdQzUW93azqLeHud1dbCsqeb3dFQZrjarH3O7KSMWlnsnGLTsZqtWZU6k0qinFMreodNXryY6iijP6fVTpalQ7t+1qVGYPmjeHnkqwq1pnXk+FRb3djapVUS2c11NpVKx3DPHw9kGWLpzL/DkVNu8Yorv4O8Ge7+V5PZWRbUP1+si/g71ZMLdRPUxgqFZnsFZnqNr4Ofp9lMDWHUPct3UnC+Z2s3ThXJYumsuc7i627Bhi8/ZBNm9v/NusdAVL5jcqapt3DLFgTjePG/73tLPKwJYd1Ouw7KDekX9j8+ZUWDS3h4W93eyq1ti4ZScL53bzuEW9EFCvN/6t1+vJUD15YNsuHnp0kEW93fRUuti8Y4h6JnO7u6jWGr//4xbPZdHcHnZWG3/jg+b1UK03vhw4ZMEcdg7V+NHPtzC3u4tjDl3AnO7H7iAbrNZ5ZGeVbbuG2D5YY06lURHeVa0BMVIl7i2qcNV6472yfbDxt1xUVP+2DzU+H7btrDJvToXDF/eOVAB3DNXYNVQrqsk9I9eoVk8Omt/D3O5G5XBo5O/d+JvvzbyeCkvm91BPqNbqHLJgDt2VruLfbGsz2sh9/QTqEKtWrcp169Yd0Gtf97rX8a//+q9Uq1W6u7s5//zzreJJUgeLiBsyc1W74yiLybSRo/3HDwd4/ad+wNfedAZPWLZoSo4pSZoaE7WPs2aQlYGBAVavXk212sjoq9Uqq1evZuPGjW2OTJKkzrN5xyDASEVCklQOsybB6+vro17f/cbiWq1GX19fmyKSJKlzbdkxBJjgSVLZzJoEb+3atQwODu62bnBwkDVr1rQpIkmSOteWHY1REXt7Zs1/FSRpRmjZp3ZEXBYR90fEzXvZ7xcjohYRv9OqWAD6+/vJTIbnCHrf+95HZtLf39/K00qSVEpbdwxx0PweYqKxwCVJHaeVX8t9DHjBRDtERAV4D/C1Fsaxm4ULFwLw6KOPTtcpJUkqnc3bh+yeKUkl1LIELzOvAx7ay25/BnyBxyZ2bbm5c+fS1dVlgidJ0gSGJ3qWJJVL2zrWR8SRwG8CH5rm87JgwQIeeeSR6TytJEmlYoInSeXUzjun/wm4cF8mbo2I8yNiXUSsG76HbjIWLlxoBU+SpAls2THEEhM8SSqd7jaeexVwZXHz9mHACyOimplfHr1jZl4KXAqNSVwne+IFCxaY4EmSNIEt24dYbIInSaXTtgQvM48bfhwRHwP+fazkrhXsoilJ0vhq9WTbrqpdNCWphFqW4EXEp4EzgcMiYj3wTqAHIDOn9b670eyiKUnS+LY6ybkklVbLErzMfPl+7PsHrYpjLAsWLGDLli3TeUpJkkpjS5HgLZlvgidJZdPOQVbaxi6akiSNb7MVPEkqrVmb4NlFU5KksW0xwZOk0pqVCZ734EmSND4TPEkqr1mZ4NlFU5Kk8Y0keN6DJ0mlM2sTvB07dlCv19sdiiRJHWfL9kHACp4kldGsTPAWLlwIwPbt29sciSRJnWfLjiF6e7qY211pdyiSpP00KxO8BQsWANhNU5KkMWzZMWT1TpJKalYneA60IknSnrbsGGLJvDntDkOSdABmZYI33EXTBE+SpD1t3m4FT5LKalYmeHbRlCRpfFt2DLHYBE+SSmlWJ3hW8CRJ2tNW78GTpNKalQmeXTQlSRqfg6xIUnnNygTPLpqSJI1vZ7XO/DlOkSBJZTSrEzwreJIk7S4zqdWTrq5odyiSpAMwKxM8u2hKkjS2Wj0B6DbBk6RSmpUJnhU8SZLGVi0SvIoJniSV0qxM8Lq7u5kzZ4734EmSNEo9reBJUpnNygQPGt00reBJkrQ7K3iSVG6zNsFbsGCBCZ4kSaPUalbwJKnMZnWCZxdNSZJ2N1LBq8za/yJIUqnN2k9vu2hKkrSn4VE0K2EFT5LKaNYmeHbRlCRpT9V6HbCLpiSV1axO8OyiKUnS7moOsiJJpTZrEzy7aEqStKeRic4rJniSVEazNsGLCO666y42btzY7lAkSeoYVvAkqdxmbYJ3yy23MDg4SF9fX7tDkSSpYwyPouk9eJJUTi1L8CLisoi4PyJuHmf7KyLih8WyJiJObVUsow0MDHDbbbcBsHr1aqt4kiQVHqvgzdrvgCWp1Fr56f0x4AUTbL8LeE5mPgXoAy5tYSy76evrI7PRgNVqNat4kiQVrOBJUrm1LMHLzOuAhybYviYzHy6eXg+saFUszQYGBli9ejW1Wg2AwcFBq3iSJBVqxTQJXSZ4klRKndL/4jXAV6fjRH19fdSLxmuYVTxJkhqqNSt4klRm3e0OICKeSyPB++UJ9jkfOB/g6KOPntT51q5dy+Dg4G7rBgcHWbNmzaSOK0nSTFBLR9GUpDJrawUvIp4CfAQ4OzMfHG+/zLw0M1dl5qqlS5dO6pz9/f1kJtdeey0A3/zmN8lM+vv7J3VcSZJmgpr34ElSqbUtwYuIo4EvAq/KzP+d7vMvXrwYgK1bt073qSVJ2k1E/HlE3BIRN0fEpyOiNyIOiYj/iog7ip8HT0csVefBk6RSa+U0CZ8G1gJPiIj1EfGaiLggIi4odnkHcCjwwYi4MSLWtSqWsZjgSZI6QUQcCbwBWJWZvwBUgHOBi4BrM/NE4NriecvVRu7B65Tb9CVJ+6Nl9+Bl5sv3sv2PgD9q1fn3xgRPktRBuoF5ETEEzAc2AG8Fziy2fxz4JnBhqwOxgidJ5TZrv54zwZMkdYLM/Dnw/wH3AAPAlsy8Bjg8MweKfQaAx01HPDUTPEkqtVmb4M2bN49KpWKCJ0lqq+LeurOB44AjgAUR8cr9eP35EbEuItZt2rRp0vFUi6mETPAkqZxmbYIXESxevNgET5LUbr8C3JWZmzJziMYAZM8C7ouI5QDFz/vHevFUjjQNUE9H0ZSkMpu1CR5ggidJ6gT3AM+IiPkREcDzgVuBq4Dzin3OA/5tOoIZnujcCp4klVPbJzpvJxM8SVK7ZeZ3I+LzwA+AKtAPXAosBD4bEa+hkQS+dDriGZkHr2KCJ0llZIJngidJarPMfCfwzlGrd9Go5k0rR9GUpHKzi6YJniRJI0YqeM6DJ0mlNKs/vU3wJEna3UgFL6zgSVIZmeCZ4EmSNKI2PE2C9+BJUimZ4JngSZI0otbI75wmQZJKatYneNu3b6darbY7FEmSOkLNic4lqdRmfYIHsG3btjZHIklSZ/AePEkqNxM8sJumJEmFWj3pCuiygidJpWSChwmeJEnDqvV0igRJKrFZ/QlugidJ0u5q9cT8TpLKa1Z/hJvgSZK0u2rNCp4kldms/gQ3wZMkaXf1TEfQlKQSM8HDUTQlSRpWrdedA0+SSswEDyt4kiQNq9Wt4ElSmc3qBG/hwoWACZ4kScMa9+CZ4ElSWc3qBK+rq4sFCxbwkY98hI0bN7Y7HEmS2q5WTyoVEzxJKqtZneAN+/nPf05fX1+7w5Akqe2q9aQSJniSVFazOsEbGBhg+/btAKxevdoqniRp1vMePEkqt1md4DVX7Wq1mlU8SdKs1xhFc1b/90CSSm3WfoIPDAywevVqMhOAwcFBq3iSpFmvVscKniSV2KxN8Pr6+qjX67uts4onSZrtavU63Q6yIkmlNWsTvLVr1zI4OLjbusHBQdasWdOmiCRJar+q9+BJUqm1LMGLiMsi4v6IuHmc7RER74+IOyPihxFxeqtiGUt/fz+Zydve9jYqlQr1ep3MpL+/fzrDkCSpo9TqzoMnSWXWygrex4AXTLD9LODEYjkf+JcWxjKugw8+mFqtxiOPPNKO00uS1FGq9aTLaRIkqbRaluBl5nXAQxPscjbwiWy4HlgSEctbFc94Dj74YAAefvjh6T61JEkdp1ZP78GTpBJr5z14RwL3Nj1fX6ybViZ4kiQ9pnEP3qy9RV+SSq+dn+BjfT2YY+4YcX5ErIuIdZs2bZrSIEzwJEl6TN178CSp1NqZ4K0Hjmp6vgLYMNaOmXlpZq7KzFVLly6d0iCWLFkCmOBJkgSOoilJZdfOBO8q4PeL0TSfAWzJzIHpDsIKniRJj6nV61bwJKnEult14Ij4NHAmcFhErAfeCfQAZOaHgKuBFwJ3AtuBV7cqlomY4EmS9BgreJJUbi1L8DLz5XvZnsDrW3X+fbVo0SK6urpM8CRJwnnwJKnsZv0wWV1dXSxZssQET5IkoFpLukzwJKm0Zn2CB41umiZ4kiRZwZOksjPBo5Hgbd68ud1hSJLUdrV0HjxJKjM/wbGCJ0nSMCt4klRuJniY4EmSNKxaqzuKpiSVmAkeJniSJA2zgidJ5WaCByOjaDZmbpAkafaq1pNKxQRPksrKBI9GBW9oaIjt27e3OxRJktqqVk8qYYInSWVlgkcjwQPspilJ2icRcXBEPCkijo+IGdWWVu2iKUml1t3uADpBc4K3YsWKNkcjSepEEXEQ8Hrg5cAcYBPQCxweEdcDH8zMb7QxxEmr1xu3KjhNgiSVlwkeVvAkSfvk88AngGdn5ubmDRHxVOBVEXF8Zn60HcFNhWqR4HV7D54klZYJHo8leK997Wu59tprWbZsWZsjkiR1msz81Qm23QDcMI3htERtpIJngidJZWUfDB5L8G699Vb6+vraHI0kqQwiYmlEvCsi/iEiTmh3PFOhWq8DeA+eJJWYCR6wa9cuADKT1atXs3HjxjZHJEkqgX8ArgP+E/h0m2OZElbwJKn8TPCA97///SOPa7WaVTxJ0h4i4j8j4tlNq+YAdxfL3Ekee0lEfD4ibouIWyPimRFxSET8V0TcUfw8eDLn2BdVEzxJKr1Zn+ANDAzwsY99bOT54OCgVTxJ0lheBpwdEZ+KiMcDbwfeAbwbeN0kj/0+4D8z84nAqcCtwEXAtZl5InBt8bylrOBJUvnN+gSvr6+PenHPwTCreJKk0TJzS2a+Gfg/wLuAPwFen5m/nZnfPtDjRsRi4Azgo8V5BotROs8GPl7s9nHgnAOPft8MJ3jegydJ5TXrR9Fcu3Ytg4ODu60bHBxkzZo1bYpIktSJIuJ44LXAEPCXwOOBz0bEv9OYA692gIc+nsaceqsj4lQao3G+ETg8MwcAMnMgIh43TlznA+cDHH300QcYQkPNefAkqfRm/Sd4f38/mcnv/M7v8MQnPpHMJDPp7+9vd2iSpM7yaRoDqlwPXJ6Z38rMXwe2AtdM4rjdwOnAv2TmacCj7Ed3zMy8NDNXZeaqpUuXTiKMpnnwrOBJUmnN+gRv2NKlS3nggQfaHYYkqXP1AncVy/zhlZn5ceBFkzjuemB9Zn63eP55GgnffRGxHKD4ef8kzrFPasUtC96DJ0nlZYJXOOyww3jwwQep1Q60h40kaYZ7HfD3wNuAC5o3ZOaOAz1oZm4E7o2IJxSrng/8GLgKOK9Ydx7wbwd6jn1lBU+Sym/W34M3bOnSpWQmDz30EJPt4iJJmnky8zvAd1p0+D8DPhkRc4CfAq+m8SXsZyPiNcA9wEtbdO4R1VojwesywZOk0jLBKwwndQ888IAJniRpDxHxFeDDwNcyc2jUtuOBPwDuzszL9vfYmXkjsGqMTc/f/0gPnKNoSlL52UWzcNhhhwGwadOmNkciSepQf0xjOoPbIuL7EXF1RPx3RPyURuJ3w4Ekd52kls6DJ0llZwWvMFy1M8GTJI2luFfuLcBbIuJYYDmwA/jfzNzeztimymMVPL//laSyMsErmOBJkvZVZt4N3N3mMKbc8D14VvAkqbxa+hVdRLwgIm6PiDsjYo85fSLioIj4SkTcFBG3RMSrWxnPRIa7aDpVgiRpthqp4FVM8CSprFqW4EVEBbgEOAs4BXh5RJwyarfXAz/OzFOBM4F/KEYQm3Zz5sxh8eLFVvAkSbNW1XnwJKn0WlnBexpwZ2b+NDMHgSuBs0ftk8CiiAhgIfAQUG1hTBNaunSpCZ4kaUIR8aKImJE3qQ1X8CphgidJZdXKBupI4N6m5+uLdc0+AJwMbAB+BLwxM+stjGlCS5cutYumJGlvzgXuiIj3RsTJ7Q5mKg1PdG4FT5LKq5UJ3litQ456/uvAjcARwErgAxGxeI8DRZwfEesiYl0rK2yHHXaYFTxJ0oQy85XAacBPgNURsbZopxa1ObRJq3sPniSV3j4leBGxYLg7SkScFBEviYievbxsPXBU0/MVNCp1zV4NfDEb7gTuAp44+kCZeWlmrsrMVa2chNwumpKkfZGZW4Ev0Lj9YDnwm8APIuLP2hrYJFWd6FySSm9fK3jXAb0RcSRwLY3E7GN7ec33gRMj4rhi4JRzgatG7XMP8HyAiDgceALw032MacrNmzePDRs2MDAw0K4QJEkdLiJeHBFfAv4b6AGelplnAacCb25rcJM0cg+e8+BJUmnt6yd4FJO4/hbwz5n5mzRGxhxXZlaBPwW+BtwKfDYzb4mICyLigmK3PuBZEfEjGonjhZnZtpvg+vv7yUze8Y53tCsESVLneynwfzPzKZn595l5P0DRTv5he0ObHCt4klR++zrReUTEM4FXAK/Z19dm5tXA1aPWfajp8Qbg1/YxhpYaGBhg3bp1AFxxxRX09fWxbNmyNkclSepA7wRGunpExDzg8My8OzOvbV9Yk1dzmgRJKr19reC9CXgr8KWiCnc88I2WRdUGfX19ZDa+uazVavT19bU5IklSh/oc0Dzic61YV3qOoilJ5bdPCV5m/k9mviQz31MMtvJAZr6hxbFNm4GBAVavXk212piCb2hoiNWrV7Nx48Y2RyZJ6kDdxfyuABSP57QxnilTM8GTpNLb11E0PxURiyNiAfBj4PaI+KvWhjZ9+vr6qNd3n37PKp4kaRybIuIlw08i4mxgRkyiWvMePEkqvX3tonlKMST0OTTuqTsaeFWrgppua9euZXBwcLd1g4ODrFmzpk0RSZI62AXA2yLinoi4F7gQ+JM2xzQlrOBJUvnt6yArPcW8d+cAH8jMoYgYPWl5afX39488PvzwwznnnHP48Ic/3MaIJEmdKjN/AjwjIhbSGGV6W7tjmiqPjaLpNAmSVFb7muB9GLgbuAm4LiKOAba2Kqh2WrZsmffeSZImFBG/ATyJxhyxAGTm/9PWoKaAFTxJKr99SvAy8/3A+5tW/SwintuakNpr+fLlJniSpHFFxIeA+cBzgY8AvwN8r61BTZFqzXvwJKns9nWQlYMi4h8jYl2x/AOwoMWxtcWyZcsYGBjY+46SpNnqWZn5+8DDmfk3wDOBo9oc05QYngevywRPkkprXzvZXwZsA363WLYCq1sVVDsNd9EcnhNPkqRRdhY/t0fEEcAQcFwb45ky1XpavZOkktvXe/Aen5m/3fT8byLixhbE03bLly9naGiIhx56iEMPPbTd4UiSOs9XImIJ8PfAD4AE/rWtEU2RWqb330lSye1rBW9HRPzy8JOI+CVgR2tCaq9ly5YBeB+eJGkPEdEFXJuZmzPzC8AxwBMz8x1tDm1K1GpW8CSp7PY1wbsAuCQi7o6Iu4EPMEPm/BnNBE+SNJ7MrAP/0PR8V2ZuaWNIU6pat4InSWW3TwleZt6UmacCTwGekpmnAc9raWRtsnz5cgAHWpEkjeeaiPjtGJ4fYQap1ZPuinPgSVKZ7es9eABkZvPcd38B/NOURtMBrOBJkvbiL2iMJF2NiJ1AAJmZi9sb1uRZwZOk8tuvBG+UGdkCLFq0iHnz5pngSZLGlJmL2h1Dq9Tqde/Bk6SSm0yCNyPnEYgIli9fbhdNSdKYIuKMsdZn5nXTHctUq9aTrpnX81SSZpUJE7yI2MbYiVwA81oSUQc45JBD+I//+A82btw40mVTkqTCXzU97gWeBtzADLg3vV5PuismeJJUZhPeSZ2ZizJz8RjLosycTPWvoz344INs2bKFvr6+dociSeowmfnipuVXgV8A7mt3XFPBe/AkqfwcKmuUgYEB7rnnHgBWr17tvXiSpL1ZTyPJK71a3XnwJKnsZmwV7kA1V+1qtRp9fX1ccsklbYxIktRJIuKfeez2hS5gJXBT2wKaQo0Knt/9SlKZ+SneZGBggNWrV1Or1QAYHBy0iidJGm0djXvubgDWAhdm5ivbG9LUsIInSeVngtekr6+Per2+27rhKp4kSYXPA1dk5scz85PA9RExv91BTYWugDnd/tdAksrMLppN1q5dy+Dg4G7rBgcHWbNmTZsikiR1oGuBXwEeKZ7PA64BntW2iKbIR877xXaHIEmaJL+ma9Lf309mUqvV6Onp4cILLyQz6e/vb3dokqTO0ZuZw8kdxeMZUcGTJJWfCd4Yurq6OOqoo0ZG05QkqcmjEXH68JOIeCqwo43xSJI0wi6a4zjmmGP42c9+1u4wJEmd503A5yJiQ/F8OfCy9oUjSdJjWlrBi4gXRMTtEXFnRFw0zj5nRsSNEXFLRPxPK+PZHyZ4kqSxZOb3gScCrwVeB5ycmTe0NypJkhpaluBFRAW4BDgLOAV4eUScMmqfJcAHgZdk5pOAl7Yqnv119NFHs2HDBoaGhtodiiSpg0TE64EFmXlzZv4IWBgRr2t3XJIkQWsreE8D7szMn2bmIHAlcPaofX4P+GJm3gOQmfe3MJ79cswxx5CZrF+/vt2hSJI6yx9n5ubhJ5n5MPDH7QtHkqTHtDLBOxK4t+n5+mJds5OAgyPimxFxQ0T8fgvj2S/HHHMMgN00JUmjdUXEyGzgRY+VOW2MR5KkEa1M8GKMdTnqeTfwVOA3gF8H3h4RJ+1xoIjzI2JdRKzbtGnT1Ec6huEE70/+5E/YuHHjtJxTklQKXwM+GxHPj4jnAZ8G/rPNMUmSBLQ2wVsPHNX0fAWwYYx9/jMzH83MB4DrgFNHHygzL83MVZm5aunSpS0LuNmKFSsAuOOOO+jr65uWc0qSSuFCGpOdvxZ4ffH4ryZzwIioRER/RPx78fyQiPiviLij+HnwpKOWJM0KrUzwvg+cGBHHRcQc4FzgqlH7/Bvw7Ijojoj5wNOBW1sY0z57+OGHAchMVq9ebRVPkgRAZtYz80OZ+TuZ+dvALcA/T/Kwb2T39u8i4NrMPJFGAjnmSNSSJI3WsgQvM6vAn9LoynIr8NnMvCUiLoiIC4p9bqXRreWHwPeAj2Tmza2KaX/09fUxfItFrVaziidJGhERKyPiPRFxN9AH3DaJY62gcavCR5pWnw18vHj8ceCcAz2+JGl2iczRt8V1tlWrVuW6detaeo6BgQGOP/54du7cObJu3rx5/PSnP2XZsmUtPbck6TERcUNmrmp3HADFPeLnAi8HHgQ+A7w5M4+Z5HE/D/wdsKg43osiYnNmLmna5+HM3Gs3zeloIyVJ7TdR+9jSic7Lqq+vj3q9vts6q3iSNOvdBjwfeHFm/nJm/jNQm8wBI+JFwP2TmSi9HQORSZI6lwneGNauXcvg4OBu6wYHB1mzZk2bIpIkdYDfBjYC34iIf42I5zP2iNH745eAlxRdPa8EnhcRVwD3RcRygOLnuPPEtmMgMklS5zLBG0N/fz+Zyfe+9z0AvvzlL5OZ9Pf3tzkySVK7ZOaXMvNlwBOBbwJ/DhweEf8SEb92gMd8a2auyMxjaXT//O/MfCWNQcnOK3Y7j8agZJIk7ZUJ3gROOOEEoDFVgiRJAMXUPp/MzBfRmALoRqZ+lMt3A78aEXcAv1o8lyRpr7rbHUAnO/jggzn00EO588472x2KJKkDZeZDwIeLZbLH+iaNyiCZ+SCN+/0kSdovVvD24oQTTjDBkyRJklQKJnh7ccIJJ9hFU5IkSVIpmODtxYknnsg999zDs5/9bDZu3NjucCRJkiRpXCZ4ezE80Mp3vvMd58GTJEmS1NFM8PZiyZIlAGQmq1evtoonSZIkqWOZ4O3FF7/4xZHHtVrNKp4kSZKkjmWCN4GBgQE+9alPjTwfHBy0iidJkiSpY5ngTaCvr496vb7bOqt4kiRJkjqVCd4E1q5dy+Dg4G7rBgcHWbNmTZsikiRJkqTxmeBNoL+/f2RwFYDbb7+dzKS/v7/NkUmSJEnSnkzw9sEv/MIvAHDzzTe3ORJJkiRJGp8J3j445ZRTiAh+9KMftTsUSZIkSRqXCd4+mD9/Pscccwwf+MAHHEFTkiRJUscywdsPDzzwgCNoSpIkSepYJnj7YGBggPXr1wM4D54kSZKkjmWCtw+aq3bVatUqniRJkqSOZIK3FwMDA6xevZpqtQrA0NCQVTxJkiRJHckEby/6+vqo1+u7ravValbxJEmSJHUcE7y9WLt2LYODg7utGxwcZM2aNW2KSJIkSZLGZoK3F/39/WQmmclf//VfU6lUePTRR+nv7293aJIkSZK0GxO8/bBq1SpqtRo33XRTu0ORJEmSpD2Y4O2HX/zFXwTgVa96lYOsSJIkSeo4LU3wIuIFEXF7RNwZERdNsN8vRkQtIn6nlfFM1hFHHMG8efP4yU9+4iArkiRJkjpOyxK8iKgAlwBnAacAL4+IU8bZ7z3A11oVy1TZuHEju3btApzwXJIkSVLnaWUF72nAnZn508wcBK4Ezh5jvz8DvgDc38JYpkRfXx8RAThVgiRJkqTO08oE70jg3qbn64t1IyLiSOA3gQ+1MI4pMTzhea1WAxpTJVjFkyRJktRJWpngxRjrctTzfwIuzMzahAeKOD8i1kXEuk2bNk1VfPvFCc8lSZIkdbruFh57PXBU0/MVwIZR+6wCriy6PR4GvDAiqpn55eadMvNS4FKAVatWjU4Sp4UTnkuSJEnqdK2s4H0fODEijouIOcC5wFXNO2TmcZl5bGYeC3weeN3o5K5TNE94/p73vAeAZzzjGXz1q19tc2SSJEmS1NCyBC8zq8Cf0hgd81bgs5l5S0RcEBEXtOq80+HMM88E4Lvf/a5dNCVJkiR1jJbOg5eZV2fmSZn5+Mz822LdhzJzj0FVMvMPMvPzrYxnqixfvhyAzHSgFUmSJEkdo6UJ3kz1d3/3d06XIEmSJKnjmODtp+HpEjIbY704XYIkSZKkTmGCt5+cLkGSJElSpzLB20/jTZfwP//zP22KSJIkSZIaTPD2U/N0CW95y1uICLq6unjOc57T7tAkSZIkzXImeJPw9Kc/ncykXq97H54kSZKktjPBm4Rrrrlm5LH34UmSJElqNxO8AzQwMMDHP/7xkeeOpilJkiSp3UzwDtBYo2lWq1WreJIkSZLaxgTvAI01mubQ0BBr1qxpU0SSJEmSZjsTvAM0PJrmhg0b6O7uBqC3t5evfvWrbY5MkiRJ0mxlgjdJfX19RATQqODZRVOSJElSu5jgTcLAwACrV69maGgIaIyk6UArkiRJktrFBG8SxhpoZefOnbz1rW9tU0SSpLKJiKMi4hsRcWtE3BIRbyzWHxIR/xURdxQ/D253rJKkzmeCNwljDbSSmXzlK19pU0SSpBKqAn+ZmScDzwBeHxGnABcB12bmicC1xXNJkiZkgjcJwwOtDA+20tXVuJzbt2+3m6YkaZ9k5kBm/qB4vA24FTgSOBsYnnD148A5bQlQklQqJnhTpK+vbyTB27VrF6effrpJniRpv0TEscBpwHeBwzNzABpJIPC4NoYmSSoJE7wpMDzYSrVaBaBerzMwMMBFF9mbRpK0byJiIfAF4E2ZuXU/Xnd+RKyLiHWbNm1qXYCSpFIwwZsCYw22AnDFFVdYxZMk7VVE9NBI7j6ZmV8sVt8XEcuL7cuB+8d6bWZempmrMnPV0qVLpydgSVLHMsGbAmMNtgKNaRPsqilJmkg0JlP9KHBrZv5j06argPOKx+cB/zbdsUmSyscEbwoMD7ayYcMGent7d9tmV01J0l78EvAq4HkRcWOxvBB4N/CrEXEH8KvFc0mSJtTd7gBmkom6ar773e9m2bJlbYhKktTJMvPbQIyz+fnTGYskqfys4E2hibpqOvm5JEmSpFYzwZtCE3XVvPzyy70XT5IkSVJLmeC1wFhdNa3iSZIkSWo1E7wWGK+rptMmSJIkSWqlliZ4EfGCiLg9Iu6MiD2GkoyIV0TED4tlTUSc2sp4pkt/f/+Y3TSr1aojakqSJElqmZYleBFRAS4BzgJOAV4eEaeM2u0u4DmZ+RSgD7i0VfFMt/FG1Lz88st55jOfaSVPkiRJ0pRrZQXvacCdmfnTzBwErgTObt4hM9dk5sPF0+uBFS2MZ1qN102zXq9z/fXX09fX14aoJEmSJM1krUzwjgTubXq+vlg3ntcAX21hPNNqohE1AT760Y9ayZMkSZI0pVqZ4I01aWuOuWPEc2kkeBeOs/38iFgXEes2bdo0hSG23nhdNXft2sX111/P6aefbpInSZIkaUq0MsFbDxzV9HwFsGH0ThHxFOAjwNmZ+eBYB8rMSzNzVWauWrp0aUuCbZXxumoOGxgYcOAVSZIkSVOiu4XH/j5wYkQcB/wcOBf4veYdIuJo4IvAqzLzf1sYS9v09/cD8LrXvY6PfvSjYyZ7l19+Obfffjtf+tKXWLZs2XSHKEmawYaGhli/fj07d+5sdygt19vby4oVK+jp6Wl3KJLUNi1L8DKzGhF/CnwNqACXZeYtEXFBsf1DwDuAQ4EPRgRANTNXtSqmdpqoktc88Moll1wyzZFJkmay9evXs2jRIo499liKtnZGykwefPBB1q9fz3HHHdfucCSpbVo6D15mXp2ZJ2Xm4zPzb4t1HyqSOzLzjzLz4MxcWSwzMrmDxwZdee1rXzvuN4sOvCJJmmo7d+7k0EMPndHJHUBEcOihh86KSqUkTaSlCZ72tHbtWoaGhsbc5sArkqRWmOnJ3bDZ8ntK0kRM8KZZf38/K1eunHCfgYEB3vCGN/Cc5zzHRE+SVGoPPvggK1euZOXKlSxbtowjjzxy5PlEg5ABrFu3jje84Q3TFKkkzQytHGRF49iXgVc+97nPERFcdNFF3HXXXXzmM59xABZJUukceuih3HjjjQBcfPHFLFy4kDe/+c0j26vVKt3dY/93ZNWqVaxaNWPv3pCklrCC10Z7m0IhM7n88sv51re+xUUXXWRFT5I0I/zBH/wBf/EXf8Fzn/tcLrzwQr73ve/xrGc9i9NOO41nPetZ3H777QB885vf5EUvehHQSA7/8A//kDPPPJPjjz+e97///e38FSSpY1nBa6PmSt5HPvKRMe/NG54k/fLLL6der3P66afz1a9+lTe84Q1W9SRJ++VvvnILP96wdUqPecoRi3nni5+036/73//9X77+9a9TqVTYunUr1113Hd3d3Xz961/nbW97G1/4whf2eM1tt93GN77xDbZt28YTnvCECQcuk6TZygSvA0w08Mqw4URvYGCAl73sZdxxxx1235QkldZLX/pSKpUKAFu2bOG8887jjjvuICLGbRN/4zd+g7lz5zJ37lwe97jHcd9997FixYrpDFuSOp4JXgfo7+/ntNNOG7lHYW+Gu65cccUV1Ot1LrroIm677TYiwsnSJUnjOpBKW6ssWLBg5PHb3/52nvvc5/KlL32Ju+++mzPPPHPM18ydO3fkcaVSoVqttjpMSSod78HrEMPz5O1thM1mtVqNzOSKK67gu9/9Ltdff/3IvXo33XST9+xJkkphy5YtHHnkkQB87GMfa28wklRyJngd5kATvWGf+MQn+Na3vsUrXvEKvv3tbzs4iySp473lLW/hrW99K7/0S7+0W5smSdp/kZntjmG/rFq1KtetW9fuMKbN/nTdHEulUqFWq7F8+fKRwVne//73O0iLpFKIiBsy03Hy99FYbeStt97KySef3KaIpt9s+30lzU4TtY9W8DrcvkyMPpHhb0IHBgZ46Utfyre//e3dqnvPeMYzeOYzn7lbl86BgQGrfpIkSVIJmeCVwHC3zeHlQBO+O+64g3q9zi233EK9Xufyyy8fuXevOel76lOfutvce97PJ0mSJJWDCV4JHch9emMZnnoBGEn6PvGJTzAwMDAyeMvo+/lGV/ys/EmSJEmdwwSvxJore5NN9oY135M5PErncPLXPFrncNK3L5W/m266acLEUJIkSdLUMMGbIaaqG+dEmkc2G076xqr8XX755btV/l7xildMmBiOl/yNlRhOlBBaQZQkSdJsZ4I3Q012cJb91Vz5q9fru1X+brnllpFtoxPDiaqCYyWGEyWEb33rW/nWt77F6aefPmGyOJwADieEY21rZuIoSZKk0miu+pRheepTn5qanJUrVyZQyqWrq2vk8SmnnJJdXV35pCc9KSNit21PeMITRrYN/xzedt555+UZZ5yR55133rjbbrzxxpGfy5cvz4jYbdvTn/70fMYznrHbfmeccUYODAzkhg0b9liXmSPrm/cZ3jZ6u6QGYF12QNtTlmWsNvLHP/7xvl7ulnjggQfy1FNPzVNPPTUPP/zwPOKII0ae79q1a6+v/8Y3vpHf+c539vl87f59JWk6TNQ+tr0x2t/FBK81ypz07e8SEbv9bF4qlUpGxEhieNJJJ425bXhdcwLZ1dWV55133khC2LxuOKGMiFy+fPlIctmcNI6VSO4toRxv2/7uP3rb6KR0rG2tZsKrTBO8/V06McFr9s53vjP//u//vqWv6aTfV5JaxQRPkzKbkr+pSh6bl+HKYnOFcXi/rq6ujIg88cQTd9t/eN3oCuPohHK8bfu7/2SrnK1IMsdKgid7zKmMb38S5PGqtsNGV33HOs9Y1d6xqsQTJeJlTJpN8GZmgrdu3bo844wz8vTTT89f+7Vfyw0bNmRm5vve9748+eST88lPfnK+7GUvy7vuumu3qt91112313N00u8rSa1igqeWMfmbmctYSWnztojIk046KSMiH//4x49sO+644zIi8uSTTx5Zd9JJJ42bSDZ3s+3q6sonPvGJe8QwVuV0+HVPfOIT95q4Nh9zKpPg/UmQmyu7YyWszYn0eOcZq9o7VpV4dAx76248HZXgycAEb2oTvKsvzLzshVO7XH3h3v6MI975znfme9/73nzmM5+Z999/f2ZmXnnllfnqV786MzOXL1+eO3fuzMzMhx9+eOQ1VvAkaXcTtY/R2F4eq1atynXr1rU7DE3Saaedxo033tjuMKSW6Orqol6v7/FzeFtmcsIJJ3DHHXfstv9EKpUK9Xp9t9eNPl9EMPyZ3nyeO++8k6OPPpqf/exnu2078cQTufPOOzn55JNHBkN60pOexK233sqJJ57I7bffvtu6k08+eeTn6P3H2va6172OSy65ZFLXMiJuyMxVkzrILDJWGzn8dwHgqxfBxh9N7UmXPRnOevc+7XrxxRfT3d3Ne9/7Xo4//nigMULz8uXLueaaa3jBC17AwoULOeecczjnnHNYuHAhF198MQsXLuTNb37zPp1jt99XkmaoidpHEzyVggmhVD69vb3cddddLFu27ICPYYK3f/aa4LXZxRdfTKVS4eqrr2bt2rV7bK/Valx33XVcddVVXH311dxyyy28613vMsGTpFEmah+dJkGlMHqevwNZxps2YuXKldM6pYQ0WwwODtLX19fuMNRh5s6dy6ZNm0YSvKGhoZGpc+69916e+9zn8t73vpfNmzfzyCOPsGjRIrZt29bmqCWpPEzwNGuMlyT29/dPOoE0QZT2VK/Xueyyy5xDUrvp6uri85//PBdeeCGnnnoqK1euZM2aNdRqNV75ylfy5Cc/mdNOO40///M/Z8mSJbz4xS/mS1/6EitXruRb3/pWu8OXpI7X3e4ApJmgv7+/3SFMifG6wg4nsHaT1f4aruJN9l48zQwXX3zxyOPrrrtuj+3f/va391h30kkn8cMf/rCVYUnSjGKCJ2nETElU22Wy94quXLlyj79B2e8/rdfrrFmzpt1hSJI0a7Q0wYuIFwDvAyrARzLz3aO2R7H9hcB24A8y8wetjEmSWqUVCbJJtyRJ2h8tuwcvIirAJcBZwCnAyyPilFG7nQWcWCznA//SqngkSZIkaaZr5SArTwPuzMyfZuYgcCVw9qh9zgY+UczXdz2wJCKWtzAmSZJmnbJNiXSgZsvvKUkTaWWCdyRwb9Pz9cW6/d1HkiQdoN7eXh588MEZn/xkJg8++CC9vb3tDkWS2qqV9+DFGOtGty77sg8RcT6NLpwcffTRk49MkqRZYsWKFaxfv55Nmza1O5SW6+3tZcWKFe0OQ5LaqpUJ3nrgqKbnK4ANB7APmXkpcCnAqlWrZvZXkJIkFfY2WNm+6Onp4bjjjpvy2CRJnamVXTS/D5wYEcdFxBzgXOCqUftcBfx+NDwD2JKZAy2MSZKkUtjHwcokSdpNyyp4mVmNiD8Fvkbjm8fLMvOWiLig2P4h4GoaUyTcSWOahFe3Kh5JkkpmZLAygIgYHqzsx22NSpLU0Vo6D15mXk0jiWte96Gmxwm8vpUxSJJUUmMNRPb0NsUiSSqJliZ4rXDDDTc8EBE/m+RhDgMemIp4pkmZ4i1TrFCueMsUK5Qr3jLFCuWKd7KxHjNVgZTQfg9EBjwSEbdP8ryz6f013coUb5lihXLFW6ZYoVzxlilWmFy847aPpUvwMnPpZI8REesyc9VUxDMdyhRvmWKFcsVbplihXPGWKVYoV7xlirUD7fdAZFOhTH+zMsUK5Yq3TLFCueItU6xQrnjLFCu0Lt5WDrIiSZIO3L4MViZJ0m5KV8GTJGk2GG+wsjaHJUnqcLM1wZuyrizTpEzxlilWKFe8ZYoVyhVvmWKFcsVbplg7zliDlU2DMv3NyhQrlCveMsUK5Yq3TLFCueItU6zQonijMZClJEmSJKnsvAdPkiRJkmaIWZfgRcQLIuL2iLgzIi5qdzzNIuKoiPhGRNwaEbdExBuL9RdHxM8j4sZieWG7Yx0WEXdHxI+KuNYV6w6JiP+KiDuKnwd3QJxPaLp+N0bE1oh4Uydd24i4LCLuj4ibm9aNey0j4q3F+/j2iPj1Doj17yPitoj4YUR8KSKWFOuPjYgdTdf4Q+MeeHrjHfdv34HX9jNNcd4dETcW69t6bSf4zOrI960m1sntI5SvjSxL+wid30aWqX2cIN6ObCPL1D5OEK9t5GiZOWsWGjep/wQ4HpgD3ASc0u64muJbDpxePF4E/C9wCnAx8OZ2xzdOzHcDh41a917gouLxRcB72h3nGO+DjTTmD+mYawucAZwO3Ly3a1m8L24C5gLHFe/rSptj/TWgu3j8nqZYj23er4Ou7Zh/+068tqO2/wPwjk64thN8ZnXk+9Zlwr9lR7ePRYylaiPL2D42vRc6qo0sU/s4Qbwd2UaWqX0cL95R220jM2ddBe9pwJ2Z+dPMHASuBM5uc0wjMnMgM39QPN4G3Aoc2d6oDsjZwMeLxx8HzmlfKGN6PvCTzPxZuwNplpnXAQ+NWj3etTwbuDIzd2XmXcCdNN7f02KsWDPzmsysFk+vpzFnV0cY59qOp+Ou7bCICOB3gU9PVzwTmeAzqyPft5pQR7ePMGPayE5vH6ED28gytY9QrjayTO0j2Ebuq9mW4B0J3Nv0fD0d2jhExLHAacB3i1V/WpT1L+uULh2FBK6JiBsi4vxi3eGZOQCNNzfwuLZFN7Zz2f0ff6deWxj/Wnb6e/kPga82PT8uIvoj4n8i4tntCmoMY/3tO/naPhu4LzPvaFrXEdd21GdWWd+3s1mp/jYlaSPL2D5CedrIMn/OlKGNLFv7CLaRI2ZbghdjrOu4YUQjYiHwBeBNmbkV+Bfg8cBKYIBG+blT/FJmng6cBbw+Is5od0ATicZkwS8BPles6uRrO5GOfS9HxF8DVeCTxaoB4OjMPA34C+BTEbG4XfE1Ge9v37HXFng5u//HqyOu7RifWePuOsa6Trm2s11p/jYlaiNL1T7CjGkjO/q9XJI2soztI9hGjphtCd564Kim5yuADW2KZUwR0UPjTfDJzPwiQGbel5m1zKwD/0oHdWnKzA3Fz/uBL9GI7b6IWA5Q/Ly/fRHu4SzgB5l5H3T2tS2Mdy078r0cEecBLwJekUWH8qKrwYPF4xto9Ck/qX1RNkzwt+/Ua9sN/BbwmeF1nXBtx/rMomTvWwEl+duUqY0sYfsI5WojS/c5U5Y2smztI9hGjjbbErzvAydGxHHFt1TnAle1OaYRRd/hjwK3ZuY/Nq1f3rTbbwI3j35tO0TEgohYNPyYxg3EN9O4pucVu50H/Ft7IhzTbt/udOq1bTLetbwKODci5kbEccCJwPfaEN+IiHgBcCHwkszc3rR+aURUisfH04j1p+2J8jET/O077toWfgW4LTPXD69o97Ud7zOLEr1vNaKj20coVxtZ0vYRytVGlupzpkxtZAnbR7CN3N2+jsYyUxbghTRGsfkJ8NftjmdUbL9MoxT7Q+DGYnkhcDnwo2L9VcDydsdaxHs8jdF+bgJuGb6ewKHAtcAdxc9D2h1rEdd84EHgoKZ1HXNtaTSqA8AQjW9xXjPRtQT+ungf3w6c1QGx3kmj7/jwe/dDxb6/Xbw/bgJ+ALy4Q67tuH/7Tru2xfqPAReM2ret13aCz6yOfN+67PXv2bHtYxFfadrIsrWPRWwd20aWqX2cIN6ObCPL1D6OF2+x3jayaYniYJIkSZKkkpttXTQlSZIkacYywZMkSZKkGcIET5IkSZJmCBM8SZIkSZohTPAkSZIkaYYwwZOmQUTUIuLGpuWiKTz2sRHRSXMTSZK0T2wfpanX3e4ApFliR2aubHcQkiR1GNtHaYpZwZPaKCLujoj3RMT3iuWEYv0xEXFtRPyw+Hl0sf7wiPhSRNxULM8qDlWJiH+NiFsi4pqImFfs/4aI+HFxnCvb9GtKkrRfbB+lA2eCJ02PeaO6oLysadvWzHwa8AHgn4p1HwA+kZlPAT4JvL9Y/37gfzLzVOB04JZi/YnAJZn5JGAz8NvF+ouA04rjXNCaX02SpANm+yhNscjMdscgzXgR8UhmLhxj/d3A8zLzpxHRA2zMzEMj4gFgeWYOFesHMvOwiNgErMjMXU3HOBb4r8w8sXh+IdCTme+KiP8EHgG+DHw5Mx9p8a8qSdI+s32Upp4VPKn9cpzH4+0zll1Nj2s8dn/tbwCXAE8FbogI77uVJJWF7aN0AEzwpPZ7WdPPtcXjNcC5xeNXAN8uHl8LvBYgIioRsXi8g0ZEF3BUZn4DeAuwBNjjW1JJkjqU7aN0APy2Qpoe8yLixqbn/5mZw0NBz42I79L4wuXlxbo3AJdFxF8Bm4BXF+vfCFwaEa+h8U3ka4GBcc5ZAa6IiIOAAP5vZm6eot9HkqSpYPsoTTHvwZPaqLjHYFVmPtDuWCRJ6hS2j9KBs4umJEmSJM0QVvAkSZIkaYawgidJkiRJM4QJniRJkiTNECZ4kiRJkjRDmOBJkiRJ0gxhgidJkiRJM4QJniRJkiTNEP8//CG7712mdY0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epoch')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Accuracy epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the final accuracy, if using sample data, this should be around 25% as there is no structure to the data. \n",
    "\n",
    "# because its a small batch we can just run this on the CPU\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "100*torch.mean((torch.argmax(model(eval_data),axis=1) == eval_labels).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lastly saving the model, this can be acessed again by using this code \n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(),'../Deeplearningmodels/deeplearningapproachwgv.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
