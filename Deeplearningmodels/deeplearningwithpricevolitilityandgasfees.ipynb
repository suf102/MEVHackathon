{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning approach with price volatility and Gas fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning library of choice PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for number-crunching\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Time to check that the gpu optimization is actually helping\n",
    "\n",
    "import time \n",
    "\n",
    "# Some graphing\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# use GPU\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Count the number of CPU cores available, this returns the number of threads, because most cpus have a thread count equal to twice their core count I have halved the count when multithreading\n",
    "# Set this number to one if you dont want to multi thread the process\n",
    "\n",
    "cpuCount = os.cpu_count()\n",
    "print(cpuCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the data from the CSV\n",
    "\n",
    "#This value will allow you to change how many periods back the data will consider. for example if you select 1 it will only look at the previous period, Maximum value is currently set to 8.\n",
    "#if you want to try over more data periods a greater number of datalet length will need to be prepped in the data prep folders. By default a maximum of 8\n",
    "\n",
    "dataperiods = 8\n",
    "\n",
    "df = pd.read_csv('../dataletswgv/datalet{}.csv'.format(dataperiods+1),index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mev_period0</th>\n",
       "      <th>Mev_period1</th>\n",
       "      <th>Mev_period2</th>\n",
       "      <th>Mev_period3</th>\n",
       "      <th>Mev_period4</th>\n",
       "      <th>Mev_period5</th>\n",
       "      <th>Mev_period6</th>\n",
       "      <th>Mev_period7</th>\n",
       "      <th>Mev_period8</th>\n",
       "      <th>gas_fees_period0</th>\n",
       "      <th>...</th>\n",
       "      <th>gas_fees_period8</th>\n",
       "      <th>price_volitility_period0</th>\n",
       "      <th>price_volitility_period1</th>\n",
       "      <th>price_volitility_period2</th>\n",
       "      <th>price_volitility_period3</th>\n",
       "      <th>price_volitility_period4</th>\n",
       "      <th>price_volitility_period5</th>\n",
       "      <th>price_volitility_period6</th>\n",
       "      <th>price_volitility_period7</th>\n",
       "      <th>price_volitility_period8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.198154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207764</td>\n",
       "      <td>0.946974</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.600441</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.165926</td>\n",
       "      <td>0.219165</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>0.038319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.683639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174731</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.600441</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.165926</td>\n",
       "      <td>0.219165</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.644419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.770326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701296</td>\n",
       "      <td>0.600441</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.165926</td>\n",
       "      <td>0.219165</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.644419</td>\n",
       "      <td>0.231879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.277586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507868</td>\n",
       "      <td>0.977247</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.165926</td>\n",
       "      <td>0.219165</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.644419</td>\n",
       "      <td>0.231879</td>\n",
       "      <td>0.273197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.636498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666308</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>0.165926</td>\n",
       "      <td>0.219165</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.644419</td>\n",
       "      <td>0.231879</td>\n",
       "      <td>0.273197</td>\n",
       "      <td>0.410002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062605</td>\n",
       "      <td>0.948023</td>\n",
       "      <td>0.580629</td>\n",
       "      <td>0.583344</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>0.338002</td>\n",
       "      <td>0.788882</td>\n",
       "      <td>0.632953</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.942736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.931640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.580629</td>\n",
       "      <td>0.583344</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>0.338002</td>\n",
       "      <td>0.788882</td>\n",
       "      <td>0.632953</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.686403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.217472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710180</td>\n",
       "      <td>0.583344</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>0.338002</td>\n",
       "      <td>0.788882</td>\n",
       "      <td>0.632953</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.686403</td>\n",
       "      <td>0.716951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539771</td>\n",
       "      <td>0.139111</td>\n",
       "      <td>0.338002</td>\n",
       "      <td>0.788882</td>\n",
       "      <td>0.632953</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.686403</td>\n",
       "      <td>0.716951</td>\n",
       "      <td>0.864340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109248</td>\n",
       "      <td>0.338002</td>\n",
       "      <td>0.788882</td>\n",
       "      <td>0.632953</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.686403</td>\n",
       "      <td>0.716951</td>\n",
       "      <td>0.864340</td>\n",
       "      <td>0.361918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9991 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Mev_period0  Mev_period1  Mev_period2  Mev_period3  Mev_period4  \\\n",
       "0             0.0          0.0          2.0          3.0          0.0   \n",
       "1             0.0          2.0          3.0          0.0          1.0   \n",
       "2             2.0          3.0          0.0          1.0          0.0   \n",
       "3             3.0          0.0          1.0          0.0          2.0   \n",
       "4             0.0          1.0          0.0          2.0          2.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "9986          0.0          3.0          2.0          1.0          2.0   \n",
       "9987          3.0          2.0          1.0          2.0          1.0   \n",
       "9988          2.0          1.0          2.0          1.0          0.0   \n",
       "9989          1.0          2.0          1.0          0.0          1.0   \n",
       "9990          2.0          1.0          0.0          1.0          1.0   \n",
       "\n",
       "      Mev_period5  Mev_period6  Mev_period7  Mev_period8  gas_fees_period0  \\\n",
       "0             1.0          0.0          2.0          2.0          0.198154   \n",
       "1             0.0          2.0          2.0          3.0          0.683639   \n",
       "2             2.0          2.0          3.0          2.0          0.770326   \n",
       "3             2.0          3.0          2.0          2.0          0.277586   \n",
       "4             3.0          2.0          2.0          3.0          0.636498   \n",
       "...           ...          ...          ...          ...               ...   \n",
       "9986          1.0          0.0          1.0          1.0          0.364444   \n",
       "9987          0.0          1.0          1.0          0.0          0.931640   \n",
       "9988          1.0          1.0          0.0          3.0          0.217472   \n",
       "9989          1.0          0.0          3.0          0.0          0.530968   \n",
       "9990          0.0          3.0          0.0          1.0          0.657100   \n",
       "\n",
       "      ...  gas_fees_period8  price_volitility_period0  \\\n",
       "0     ...          0.207764                  0.946974   \n",
       "1     ...          0.174731                  0.960688   \n",
       "2     ...          0.701296                  0.600441   \n",
       "3     ...          0.507868                  0.977247   \n",
       "4     ...          0.666308                  0.044735   \n",
       "...   ...               ...                       ...   \n",
       "9986  ...          0.062605                  0.948023   \n",
       "9987  ...          0.023214                  0.580629   \n",
       "9988  ...          0.710180                  0.583344   \n",
       "9989  ...          0.539771                  0.139111   \n",
       "9990  ...          0.109248                  0.338002   \n",
       "\n",
       "      price_volitility_period1  price_volitility_period2  \\\n",
       "0                     0.960688                  0.600441   \n",
       "1                     0.600441                  0.977247   \n",
       "2                     0.977247                  0.044735   \n",
       "3                     0.044735                  0.165926   \n",
       "4                     0.165926                  0.219165   \n",
       "...                        ...                       ...   \n",
       "9986                  0.580629                  0.583344   \n",
       "9987                  0.583344                  0.139111   \n",
       "9988                  0.139111                  0.338002   \n",
       "9989                  0.338002                  0.788882   \n",
       "9990                  0.788882                  0.632953   \n",
       "\n",
       "      price_volitility_period3  price_volitility_period4  \\\n",
       "0                     0.977247                  0.044735   \n",
       "1                     0.044735                  0.165926   \n",
       "2                     0.165926                  0.219165   \n",
       "3                     0.219165                  0.114442   \n",
       "4                     0.114442                  0.038319   \n",
       "...                        ...                       ...   \n",
       "9986                  0.139111                  0.338002   \n",
       "9987                  0.338002                  0.788882   \n",
       "9988                  0.788882                  0.632953   \n",
       "9989                  0.632953                  0.093129   \n",
       "9990                  0.093129                  0.942736   \n",
       "\n",
       "      price_volitility_period5  price_volitility_period6  \\\n",
       "0                     0.165926                  0.219165   \n",
       "1                     0.219165                  0.114442   \n",
       "2                     0.114442                  0.038319   \n",
       "3                     0.038319                  0.644419   \n",
       "4                     0.644419                  0.231879   \n",
       "...                        ...                       ...   \n",
       "9986                  0.788882                  0.632953   \n",
       "9987                  0.632953                  0.093129   \n",
       "9988                  0.093129                  0.942736   \n",
       "9989                  0.942736                  0.686403   \n",
       "9990                  0.686403                  0.716951   \n",
       "\n",
       "      price_volitility_period7  price_volitility_period8  \n",
       "0                     0.114442                  0.038319  \n",
       "1                     0.038319                  0.644419  \n",
       "2                     0.644419                  0.231879  \n",
       "3                     0.231879                  0.273197  \n",
       "4                     0.273197                  0.410002  \n",
       "...                        ...                       ...  \n",
       "9986                  0.093129                  0.942736  \n",
       "9987                  0.942736                  0.686403  \n",
       "9988                  0.686403                  0.716951  \n",
       "9989                  0.716951                  0.864340  \n",
       "9990                  0.864340                  0.361918  \n",
       "\n",
       "[9991 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataheaders = list(df.columns)\n",
    "dataheaders.remove('Mev_period{}'.format(int((df.shape[1]/3-1))))\n",
    "dataheaders.remove('gas_fees_period{}'.format(int((df.shape[1]/3-1))))\n",
    "dataheaders.remove('price_volitility_period{}'.format(int((df.shape[1]/3-1))))\n",
    "\n",
    "data = torch.Tensor(df[dataheaders].values).type(torch.float)\n",
    "labels = torch.Tensor(df['Mev_period{}'.format(int((df.shape[1]/3)-1))].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 2.0000,  ..., 0.1659, 0.2192, 0.1144],\n",
      "        [0.0000, 2.0000, 3.0000,  ..., 0.2192, 0.1144, 0.0383],\n",
      "        [2.0000, 3.0000, 0.0000,  ..., 0.1144, 0.0383, 0.6444],\n",
      "        ...,\n",
      "        [2.0000, 1.0000, 2.0000,  ..., 0.0931, 0.9427, 0.6864],\n",
      "        [1.0000, 2.0000, 1.0000,  ..., 0.9427, 0.6864, 0.7170],\n",
      "        [2.0000, 1.0000, 0.0000,  ..., 0.6864, 0.7170, 0.8643]])\n",
      "tensor([2, 3, 2,  ..., 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to split the data into three parts, evaluation, test and training data. I do this with two instances of train_test_split, just for the sake of convinience.\n",
    "\n",
    "traintemp_data,eval_data, traintemp_labels,eval_labels = train_test_split(data, labels, test_size=.01)\n",
    "\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(traintemp_data, traintemp_labels, test_size=.011)\n",
    "\n",
    "#then we are going to pass the data to the Pytorch data loader, this is going to allow us to split it into mini batches that will be run through the model.\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "train_data = train_data\n",
    "test_data = test_data\n",
    "\n",
    "#Best to keep batches to powers of two for speed reasons adjust as needed for your own memory constraints \n",
    "x = 11\n",
    "batches   = 2**x\n",
    "train_loader = DataLoader(train_data,batch_size=batches,shuffle=True,drop_last=True, num_workers=(int(cpuCount/2)))\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANN(nn.Module):\n",
    "\tdef __init__(self, Input_dim, Output_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### input layer\n",
    "\t\tself.input = nn.Linear(Input_dim,64)\n",
    "\t\t\n",
    "\t\t### hidden layers\n",
    "\t\tself.bnorm1 \t= nn.BatchNorm1d(64) \n",
    "\t\tself.hidden1    = nn.Linear(64,256)\n",
    "\t\tself.bnorm2 \t= nn.BatchNorm1d(256) \n",
    "\t\tself.hidden2    = nn.Linear(256,256)\n",
    "\t\tself.bnorm3 \t= nn.BatchNorm1d(256)\n",
    "\t\tself.hidden3    = nn.Linear(256,256)\n",
    "\t\tself.bnorm4 \t= nn.BatchNorm1d(256)\n",
    "\t\tself.hidden4    = nn.Linear(256,256)\n",
    "\n",
    "\t\t### output layer\n",
    "\t\tself.output = nn.Linear(256,Output_dim)\n",
    "\t\n",
    "\t# forward pass\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t# input (x starts off normalized)\n",
    "\t\tx = F.relu( self.input(x) )\n",
    "\n",
    "\n",
    "\t\t# hidden layer 1\n",
    "\t\tx = self.bnorm1(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden1(x) )      # linear function and activation function\n",
    "\n",
    "\t\t# hidden layer 2\n",
    "\t\tx = self.bnorm2(x) # batchnorm\n",
    "\t\tx = F.relu( self.hidden2(x) )      # linear function and activation function\n",
    "\t\t\n",
    "\t\t# hidden layer 3\n",
    "\t\tx = self.bnorm3(x)\n",
    "\t\tx = F.relu( self.hidden3(x) )      # linear function and activation function\n",
    "  \n",
    "\t\t# hidden layer 4\n",
    "\t\tx = self.bnorm4(x)\n",
    "\t\tx = F.relu( self.hidden4(x) )\n",
    "\n",
    "\t\t# output layer\n",
    "\t\treturn self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainthemodel(learning):\n",
    "\t\n",
    "\t# Loss function and optimizer, I chose cross entropy loss as it is best for classification problems. \n",
    "\tlossfun = nn.CrossEntropyLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(),lr=learning)\n",
    "\t\n",
    "\t#initialize losses\n",
    "\tlosses = torch.zeros(numofepochs)\n",
    "\ttrainAcc = []\n",
    "\ttestAcc = []\n",
    "\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t#now lets actually loop over the training epochs to train the model\n",
    "\tfor epoch in range(numofepochs):\n",
    "\t\t\n",
    "\t\t# switch on training mode\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# loop over training data batches\n",
    "\t\tbatchAcc  = []\n",
    "\t\tbatchLoss = []\n",
    "\t\tfor X,y in train_loader:\n",
    "\n",
    "\t\t\tX = X.to(device)\n",
    "\t\t\ty = y.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# forward pass and loss\n",
    "\t\t\tyHat = model(X)\n",
    "\t\t\tloss = lossfun(yHat,y)\n",
    "\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# loss from this batch\n",
    "\t\t\tbatchLoss.append(loss.item())\n",
    "\n",
    "\t\t\tyHat = yHat.cpu()\n",
    "\t\t\ty = y.cpu()\n",
    "\n",
    "\t\t\t# compute training accuracy for this batch\n",
    "\t\t\tbatchAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()).item() )\n",
    "\t\t\t\n",
    "\t\t# now that we've trained through the batches, get their average training accuracy\n",
    "\t\ttrainAcc.append( np.mean(batchAcc)) \n",
    "\n",
    "\t\t# and get average losses across the batches\n",
    "\t\tlosses[epoch] = np.mean(batchLoss)\n",
    "\t\t\n",
    "\t\t### test accuracy\n",
    "\n",
    "\t\t# Lets turn eval back on so we dont overfit with the test data \n",
    "\t\tmodel.eval()\n",
    "\t\tX,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "\t\tX = X.to(device)\n",
    "\t\ty = y.to(device)  \n",
    "\n",
    "\t\twith torch.no_grad(): # deactivates autograd\n",
    "\t\t\tyHat = model(X)\n",
    "   \n",
    "\t\tyHat = yHat.cpu()\n",
    "\t\ty = y.cpu()   \n",
    "\n",
    "\t\ttestAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1) == y).float()) )\n",
    "\n",
    "\t\tprint('epoch {} done at time {} '.format(epoch,time.perf_counter()))\n",
    "\n",
    "\n",
    "\t# function output\n",
    "\treturn trainAcc,testAcc,losses,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done at time 8.0775743 \n",
      "epoch 1 done at time 9.7449032 \n",
      "epoch 2 done at time 11.449819 \n",
      "epoch 3 done at time 13.1056497 \n",
      "epoch 4 done at time 14.7984394 \n",
      "epoch 5 done at time 16.4784807 \n",
      "epoch 6 done at time 18.2133128 \n",
      "epoch 7 done at time 19.9077097 \n",
      "epoch 8 done at time 21.5874912 \n",
      "epoch 9 done at time 23.3958784 \n",
      "epoch 10 done at time 25.0984109 \n",
      "epoch 11 done at time 26.7645336 \n",
      "epoch 12 done at time 28.4416072 \n",
      "epoch 13 done at time 30.0969608 \n",
      "epoch 14 done at time 31.8324483 \n",
      "epoch 15 done at time 33.5245616 \n",
      "epoch 16 done at time 35.2881419 \n",
      "epoch 17 done at time 37.0970845 \n",
      "epoch 18 done at time 38.7994355 \n",
      "epoch 19 done at time 40.5970835 \n",
      "epoch 20 done at time 42.2700982 \n",
      "epoch 21 done at time 43.9920321 \n",
      "epoch 22 done at time 45.7432904 \n",
      "epoch 23 done at time 47.3916017 \n",
      "epoch 24 done at time 49.1352093 \n",
      "epoch 25 done at time 50.8665165 \n",
      "epoch 26 done at time 52.5830344 \n",
      "epoch 27 done at time 54.3320535 \n",
      "epoch 28 done at time 55.9964854 \n",
      "epoch 29 done at time 57.7345884 \n",
      "epoch 30 done at time 59.4331686 \n",
      "epoch 31 done at time 61.1813463 \n",
      "epoch 32 done at time 62.8701695 \n",
      "epoch 33 done at time 64.5793973 \n",
      "epoch 34 done at time 66.317047 \n",
      "epoch 35 done at time 68.132677 \n",
      "epoch 36 done at time 69.865943 \n",
      "epoch 37 done at time 71.6528657 \n",
      "epoch 38 done at time 73.3160201 \n",
      "epoch 39 done at time 75.104 \n",
      "epoch 40 done at time 76.8985655 \n",
      "epoch 41 done at time 78.6157211 \n",
      "epoch 42 done at time 80.4063034 \n",
      "epoch 43 done at time 82.0550858 \n",
      "epoch 44 done at time 83.7809767 \n",
      "epoch 45 done at time 85.430813 \n",
      "epoch 46 done at time 87.2251121 \n",
      "epoch 47 done at time 88.9817886 \n",
      "epoch 48 done at time 90.7481465 \n",
      "epoch 49 done at time 92.4360341 \n",
      "epoch 50 done at time 94.1457795 \n",
      "epoch 51 done at time 95.9091566 \n",
      "epoch 52 done at time 97.6080374 \n",
      "epoch 53 done at time 99.3040439 \n",
      "epoch 54 done at time 101.0147943 \n",
      "epoch 55 done at time 102.7886202 \n",
      "epoch 56 done at time 104.8781369 \n",
      "epoch 57 done at time 106.9078353 \n",
      "epoch 58 done at time 108.6596706 \n",
      "epoch 59 done at time 110.6179472 \n",
      "epoch 60 done at time 112.7288586 \n",
      "epoch 61 done at time 114.6056869 \n",
      "epoch 62 done at time 116.3480607 \n",
      "epoch 63 done at time 118.0615933 \n",
      "epoch 64 done at time 119.7476207 \n",
      "epoch 65 done at time 121.4368876 \n",
      "epoch 66 done at time 123.242619 \n",
      "epoch 67 done at time 125.0250162 \n",
      "epoch 68 done at time 126.7175745 \n",
      "epoch 69 done at time 128.4490525 \n",
      "epoch 70 done at time 130.2453425 \n",
      "epoch 71 done at time 131.9820663 \n",
      "epoch 72 done at time 133.7121233 \n",
      "epoch 73 done at time 135.5041847 \n",
      "epoch 74 done at time 137.3829247 \n",
      "epoch 75 done at time 139.1484052 \n",
      "epoch 76 done at time 140.9208657 \n",
      "epoch 77 done at time 142.6712934 \n",
      "epoch 78 done at time 144.401112 \n",
      "epoch 79 done at time 146.0912578 \n",
      "epoch 80 done at time 147.8701389 \n",
      "epoch 81 done at time 149.5746184 \n",
      "epoch 82 done at time 151.3951453 \n",
      "epoch 83 done at time 153.0728997 \n",
      "epoch 84 done at time 154.8366023 \n",
      "epoch 85 done at time 156.4908855 \n",
      "epoch 86 done at time 158.2863377 \n",
      "epoch 87 done at time 160.0386886 \n",
      "epoch 88 done at time 161.80451 \n",
      "epoch 89 done at time 163.5826187 \n",
      "epoch 90 done at time 165.3339185 \n",
      "epoch 91 done at time 167.0232881 \n",
      "epoch 92 done at time 168.7306897 \n",
      "epoch 93 done at time 170.5194789 \n",
      "epoch 94 done at time 172.249859 \n",
      "epoch 95 done at time 173.9962488 \n",
      "epoch 96 done at time 175.9148806 \n",
      "epoch 97 done at time 177.7210074 \n",
      "epoch 98 done at time 179.512515 \n",
      "epoch 99 done at time 181.2002002 \n",
      "epoch 100 done at time 182.9558862 \n",
      "epoch 101 done at time 184.7571046 \n",
      "epoch 102 done at time 186.4678119 \n",
      "epoch 103 done at time 188.1178842 \n",
      "epoch 104 done at time 189.8192143 \n",
      "epoch 105 done at time 191.5385894 \n",
      "epoch 106 done at time 193.24134 \n",
      "epoch 107 done at time 194.9844764 \n",
      "epoch 108 done at time 196.7161633 \n",
      "epoch 109 done at time 198.3801801 \n",
      "epoch 110 done at time 200.0807049 \n",
      "epoch 111 done at time 201.8321676 \n",
      "epoch 112 done at time 203.5842044 \n",
      "epoch 113 done at time 205.2864334 \n",
      "epoch 114 done at time 207.0836426 \n",
      "epoch 115 done at time 208.8315725 \n",
      "epoch 116 done at time 210.5971901 \n",
      "epoch 117 done at time 212.3639257 \n",
      "epoch 118 done at time 214.1318343 \n",
      "epoch 119 done at time 215.8961242 \n",
      "epoch 120 done at time 217.6405196 \n",
      "epoch 121 done at time 219.3301187 \n",
      "epoch 122 done at time 221.1388818 \n",
      "epoch 123 done at time 223.0248821 \n",
      "epoch 124 done at time 224.715524 \n",
      "epoch 125 done at time 226.4514758 \n",
      "epoch 126 done at time 228.1910411 \n",
      "epoch 127 done at time 229.9194974 \n",
      "epoch 128 done at time 231.9744254 \n",
      "epoch 129 done at time 233.7047319 \n",
      "epoch 130 done at time 235.3893673 \n",
      "epoch 131 done at time 237.0793618 \n",
      "epoch 132 done at time 238.7710863 \n",
      "epoch 133 done at time 240.4466501 \n",
      "epoch 134 done at time 242.1121403 \n",
      "epoch 135 done at time 243.7748081 \n",
      "epoch 136 done at time 245.4255799 \n",
      "epoch 137 done at time 247.101942 \n",
      "epoch 138 done at time 248.8161881 \n",
      "epoch 139 done at time 250.5672492 \n",
      "epoch 140 done at time 252.3022149 \n",
      "epoch 141 done at time 254.0315237 \n",
      "epoch 142 done at time 255.7420983 \n",
      "epoch 143 done at time 257.4239993 \n",
      "epoch 144 done at time 259.1046192 \n",
      "epoch 145 done at time 260.8208697 \n",
      "epoch 146 done at time 262.5369359 \n",
      "epoch 147 done at time 264.2791639 \n",
      "epoch 148 done at time 265.9984555 \n",
      "epoch 149 done at time 267.7965918 \n",
      "epoch 150 done at time 269.5420749 \n",
      "epoch 151 done at time 271.3689064 \n",
      "epoch 152 done at time 273.1411866 \n",
      "epoch 153 done at time 274.8639142 \n",
      "epoch 154 done at time 276.6485647 \n",
      "epoch 155 done at time 278.3394496 \n",
      "epoch 156 done at time 280.1682958 \n",
      "epoch 157 done at time 281.9227052 \n",
      "epoch 158 done at time 283.6843132 \n",
      "epoch 159 done at time 285.687474 \n",
      "epoch 160 done at time 287.3696752 \n",
      "epoch 161 done at time 289.4352507 \n",
      "epoch 162 done at time 291.0999045 \n",
      "epoch 163 done at time 292.8340334 \n",
      "epoch 164 done at time 294.6226223 \n",
      "epoch 165 done at time 296.3581785 \n",
      "epoch 166 done at time 297.9784362 \n",
      "epoch 167 done at time 299.7134168 \n",
      "epoch 168 done at time 301.4587694 \n",
      "epoch 169 done at time 303.0975008 \n",
      "epoch 170 done at time 304.8035286 \n",
      "epoch 171 done at time 306.4882425 \n",
      "epoch 172 done at time 308.2314972 \n",
      "epoch 173 done at time 309.8959688 \n",
      "epoch 174 done at time 311.5889143 \n",
      "epoch 175 done at time 313.2476961 \n",
      "epoch 176 done at time 314.9171315 \n",
      "epoch 177 done at time 316.6884728 \n",
      "epoch 178 done at time 318.3692063 \n",
      "epoch 179 done at time 320.0581083 \n",
      "epoch 180 done at time 321.7545219 \n",
      "epoch 181 done at time 323.4627572 \n",
      "epoch 182 done at time 325.2388018 \n",
      "epoch 183 done at time 327.0202213 \n"
     ]
    }
   ],
   "source": [
    "# Time to run the model, first we need to input parameters, you might want to change the number of epochs if it isn't reaching the level of accuracy desired.\n",
    "\n",
    "input_dim = df.shape[1]-3\n",
    "output_dim = 4\n",
    "numofepochs = 1000\n",
    "learningrate = 0.01\n",
    "\n",
    "\n",
    "model = ANN(Input_dim = input_dim,Output_dim = output_dim)\n",
    "trainAcc,testAcc,losses,model = trainthemodel(learningrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(losses,'k^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses over epoch')\n",
    "\n",
    "ax[1].plot(trainAcc,)\n",
    "ax[1].plot(testAcc,)\n",
    "ax[1].set_title('Accuracy epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([0,103])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the final accuracy, if using sample data, this should be around 25% as there is no structure to the data. \n",
    "\n",
    "# because its a small batch we can just run this on the CPU\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "100*torch.mean((torch.argmax(model(eval_data),axis=1) == eval_labels).float()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../Deeplearningmodels/deeplearningapproachwgv.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
