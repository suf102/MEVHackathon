{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data partitioning\n",
    "\n",
    "In this notebook we will take the conditioned data that we have, the data should be in the form of a two collumn csv that has the block numvber on the left and the MEV quantity sorted in to none = 0, low = 1, medium = 2, and high =3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from numba import jit\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[          nan           nan           nan]\n",
      " [0.0000000e+00 1.1834049e+07 4.0000000e+00]\n",
      " [1.0000000e+00 1.1834050e+07 0.0000000e+00]\n",
      " ...\n",
      " [3.1529040e+06 1.4986953e+07 0.0000000e+00]\n",
      " [3.1529050e+06 1.4986954e+07 4.0000000e+00]\n",
      " [3.1529060e+06 1.4986955e+07 4.0000000e+00]]\n",
      "(3152908, 3)\n",
      "[[1.1834049e+07 4.0000000e+00]\n",
      " [1.1834050e+07 0.0000000e+00]\n",
      " [1.1834051e+07 0.0000000e+00]\n",
      " ...\n",
      " [1.4986953e+07 0.0000000e+00]\n",
      " [1.4986954e+07 4.0000000e+00]\n",
      " [1.4986955e+07 4.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#let pull the data in and so that we can check how big the files are. \n",
    "\n",
    "my_data = genfromtxt('..\\Ethdata\\MEVcategory.csv', delimiter=',')\n",
    "print(my_data)\n",
    "print(np.shape(my_data))\n",
    "my_data = np.delete(my_data, 0,0) #just to get rid of the index row\n",
    "my_data = np.delete(my_data,0,1) #and the nan values at the top\n",
    "print(my_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datalet2': array([[4, 0],\n",
      "       [0, 0],\n",
      "       [0, 0],\n",
      "       ...,\n",
      "       [0, 4],\n",
      "       [4, 0],\n",
      "       [0, 4]]), 'datalet3': array([[4, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 4],\n",
      "       [0, 4, 0],\n",
      "       [4, 0, 4]]), 'datalet4': array([[4, 0, 0, 0],\n",
      "       [0, 0, 0, 0],\n",
      "       [0, 0, 0, 0],\n",
      "       ...,\n",
      "       [4, 0, 0, 4],\n",
      "       [0, 0, 4, 0],\n",
      "       [0, 4, 0, 4]]), 'datalet5': array([[4, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       ...,\n",
      "       [4, 4, 0, 0, 4],\n",
      "       [4, 0, 0, 4, 0],\n",
      "       [0, 0, 4, 0, 4]]), 'datalet6': array([[4, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 4, 4, 0, 0, 4],\n",
      "       [4, 4, 0, 0, 4, 0],\n",
      "       [4, 0, 0, 4, 0, 4]]), 'datalet7': array([[4, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 4, ..., 0, 0, 4],\n",
      "       [0, 4, 4, ..., 0, 4, 0],\n",
      "       [4, 4, 0, ..., 4, 0, 4]]), 'datalet8': array([[4, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [4, 0, 0, ..., 0, 0, 4],\n",
      "       [0, 0, 4, ..., 0, 4, 0],\n",
      "       [0, 4, 4, ..., 4, 0, 4]]), 'datalet9': array([[4, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [4, 4, 0, ..., 0, 0, 4],\n",
      "       [4, 0, 0, ..., 0, 4, 0],\n",
      "       [0, 0, 4, ..., 4, 0, 4]]), 'datalet10': array([[4, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [3, 4, 4, ..., 0, 0, 4],\n",
      "       [4, 4, 0, ..., 0, 4, 0],\n",
      "       [4, 0, 0, ..., 4, 0, 4]])}\n"
     ]
    }
   ],
   "source": [
    "#what this function will do is take one data point and the next two after, the next one after will be used with that one for the x part of the data and the third will be the y position, \n",
    "#the answer that we are asking here is can you predict the next MEV quantity based on the two before. This might look a little arduous, however doing it this way all in np, is much \n",
    "#much mich faster than trying to do it any other way, a for loop over 10000000 data entries would just take way too long. \n",
    "\n",
    "\n",
    "#this parameter will set what the maximum datalet length will be i.e. how many blocks back we want to go to predict the MEV level of the next block\n",
    "\n",
    "startrange = 2\n",
    "\n",
    "endrange = 10\n",
    "\n",
    "#start by making a dictionary to hold all this data\n",
    "\n",
    "datalets = {}\n",
    "\n",
    "# Then we loop over, the default is set from 2-10, this is the length of hte data length, the x vector will be one less than this. \n",
    "\n",
    "for i in range(startrange,endrange+1):\n",
    "    \n",
    "    # Start with an all zeros np array so we can get started, it will have i columns because that is the number of \n",
    "    \n",
    "    datalet = np.zeros((np.shape(my_data)[0],i))\n",
    "    \n",
    "    #this will clone the price column, delete the top entry to shift it all down, put some nonsense in at the bottom then add it to the datalets dictionary.\n",
    "    \n",
    "    for j in range(i):\n",
    "        data = np.delete(my_data[:,1],range(j),0)\n",
    "        data = np.append(data,range(j))\n",
    "        data = data.astype(int)\n",
    "        datalet[:,j] = data\n",
    "        \n",
    "    # to not screw up the results this next couple of lines get rid of the nonsence that we added in the bottom rows. \n",
    "        \n",
    "    datalet = datalet.astype(int)\n",
    "    datalet = np.delete(datalet,range(i*(-1),0),0)\n",
    "        \n",
    "    datalets[\"datalet{}\".format(i)] = datalet\n",
    "\n",
    "print(datalets)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving these datalets as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in datalets.items():\n",
    "    headers = []\n",
    "    for i in range(np.shape(y)[1]):\n",
    "        headers.append('period{}'.format(i))\n",
    "    headerstr = ','.join(map(str,headers))\n",
    "    np.savetxt(\"..\\datalets\\{}.csv\".format(x), y, delimiter=\",\", header= headerstr, comments = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
